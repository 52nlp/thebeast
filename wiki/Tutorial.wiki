#summary Tutorial using Semantic Role Labelling

= Introduction =

This page will give a brief tutorial that covers some of thebeast's features. As task we look at a vanilla version of Semantic Role Labelling (SRL): finding out which are arguments of a verb and what roles do they play. For example, in

 Haag plays Elianti

Haag is the agent ("A0") and Elianti the patient ("A1") of the verb "play". 

The MLN we describe here can be found in {{{examples/srl/toy.pml}}}. 

= Defining Types =
First we define the types of constants that will appear in our world. We will save these definitions in an external file {{{srl-types.pml}}} (this is not mandatory, they could also be in {{{toy.pml}}}.
{{{
type Word: ...;
type Pos: ...;
type Role: ...;
}}}
Here {{{Word}}} is a set of words, {{{Pos}}} a set of Part Of Speech tags and {{{Role}}} the set of possible roles arguments can have. 

Note that Instead of explicitely defining the constants we use the "..." notation to indicate that constants should be added when seen in the data. We could also write 
{{{
type Role: "A0","A1";
}}}

In {{{toy.pml}}} we include the type definitions by calling
{{{
include "srl-types.pml";
}}}

=Defining Predicates= 
We will the describe a sentence and possible role labelling using logical predicates. To define these we start a new file called {{{srl.pml}}}. Again it is possible to specify these predicates inline in the top level {{{toy.pml}}} file.
{{{
/* Predicate definitions */

// The word predicate maps token indices (Int=Integer) to words: word(t,w) means that token t has word w.
predicate word: Int x Word;

// The pos predicate maps token indices to Part of Speech tags. I.e: pos(t,p) means that token t has tag p.
predicate pos: Int x Pos;

// The role predicate role(p,a,r) indicates that the token a is an argument of token p with role r.
predicate role: Int x Int x Role;

// the unique predicate denotes roles which cannot appear more than once (like "A0")
predicate unique: Role;
}}} 

= Defining Formulae =
We are now ready to define some MLN formulae that describes our intuition of Semantic Role Labelling. 

== Global Formulae ==
The main aim of languages such as Markov Logic is to allow in incorporation of global correlations between 
decisions that go beyond the linear dependencies of linear-chain CRFs etc. In this tutorial we will use two of such correlations and save them in {{{srl-global.pml}}}.
{{{
// this formula ensures that an argument cannot be have more than one role wrt to
// one predicate. 
factor: for Int p, Int a if word(p,_) & word(a,_) : |Role r: role(p,a,r)| <=1;

// this formula ensures that an SRL predicate cannot have more than one argument with
// a unique role, such as "A0"
factor: for Int p, Role r if word(p,_) & unique(r) : |Int a: word(a,_) & role(p,a,r)| <=1;
}}}
Note that both of these formulae are deterministic. In future tutorials we will also introduce global soft formulae.
Also note that {{{thebeast}}} allows cardinality constraints. 

We include this file using 
{{{
include "srl-global.pml";
}}}
in {{{srl.pml}}}

== Local Formulae ==
We also define a set of formulae that only consider a single hidden {{{role(p,a,r)}}} decision. In Natural Language Processing such formulae are often extremely powerful and not many global ones are actually needed. We write these formulae into {{{srl-local.pml}}}.
{{{
// This formula tests whether the argument is to the left of the (SRL) predicate
weight w_left: Role -> Double;
factor: for Int p, Int a, Role r
  if word(p,_) & word(a,_) & a < p add [role(p,a,r)] * w_left(r);

// This formula checks the POS tag of the predicate and argument token
weight w_pos_pa: Pos x Pos x Role -> Double;
factor: for Int p, Int a, Pos p_pos, Pos a_pos, Role r
  if pos(p,p_pos) & pos(a,a_pos) add [role(p,a,r)] * w_pos_pa(p_pos,a_pos,r);

// This formula checks the POS tag of the predicate token
weight w_pos_p: Pos x Role -> Double;
factor: for Int p, Int a, Pos p_pos, Role r
  if pos(p,p_pos) & pos(a,_) add [role(p,a,r)] * w_pos_p(p_pos,r);
}}}
By default features are only considered if they have been seen at least once in the training set. For the formula above this means that we cannot learn from negative examples. The command below enforces that every possible POS tag & role combination (argument to w_pos_p) can have a nonzero weight:

{{{
set collector.all.w_pos_p = true;
}}}

Finally we add 
{{{
include "srl-global.pml";
}}}
to {{{srl.pml}}}. 

= Defining Hidden, Observed and Global Predicates =
Now we specify which information has to be inferred (hidden), which is given but different for every possible world (observed) and which is static and holds for all possible worlds (global). 
{{{
observed: word,pos;
hidden: role;
global: unique;
}}}

This concludes our model of the SRL task in {{{srl.pml}}} 

= Defining Global Static Data =
Which arguments roles have to be unique with respect to one verb is information that is static for all worlds. Thus we defined {{{unique}}} as a global predicate. In order to specify which roles are supposed to be unique we create a file {{{global.atoms}}} with 
{{{
>unique
"A0"
"A1"
}}}
Note that we don't need {{{>>}}} lines because there is only one world we need to specify.

We load this data by calling 
{{{
load global from "global.atoms";
}}}
in {{{toy.pml}}}


= Adding Training Data ==
Now we will add some training data in order to train the weights of our MLN. In Markov Logic data is defined in terms of possible worlds: collections of ground atoms. We write a set of possible worlds into a {{{train.atoms}}} file:
{{{
>>
>word
0 "Haag"
1 "plays"
2 "Elianti"
3 "."

>role
1 0 "A0"
1 2 "A1"

>pos
0 "NNP"
1 "VBZ"
2 "NNP"
3 "P"

>>
>word
0 "He"
1 "plays"
2 "the"
3 "fool"
4 "."

>role
1 0 "A0"
1 3 "A1"

>pos
0 "NNP"
1 "VBZ"
2 "DT"
3 "NN"
4 "P"
}}}

We load this data by calling 
{{{
load corpus from "train.atoms";
}}}
in {{{toy.pml}}}

= Instantiating Formulae =
Different weights are assigned to different groundings of our formulae. For example in the second local formulae we use different weights for different combinations of POS tags of the verb and argument and the role given to the argument. Before we learn this weights we would like to define which of these weights are allowed to be nonzero. For example, it is common to only use features that have been seen in training data at least once (or {{{k}}} times).  

To define which weights are allowed to be nonzero we collect all active features from the training set and count them using
{{{
collect;
}}}

Depending on the settings we made before (such as the {{{set collect.all...}}} statement in {{{srl-local.pml}}} we now get a set of weights which are allowed to be nonzero. We can print them out by calling
{{{
print weights;
}}}





