#summary Tutorial using Bilingual Alignment
#labels
= Introduction =

This page will give a brief tutorial that covers some of thebeast's features. As task we look at a vanilla version of bilingual alignment: determine which tokens of a sentence in one language translate to which tokens in the same sentence written in another language.

For example, for the English sentence

 Haag plays Elianti

and the German sentence 

 Haag spielt Elianti

an alignment could look like:

||         || Haag || plays || Elianti ||
|| Haag    ||  x   ||       ||         ||
|| spielt  ||      ||   x   ||         ||
|| Elianti ||      ||       ||    x    ||


The MLN we describe here can be found in {{{examples/align/toy.pml}}}. 

While {{{toy.pml}}} helps to get a rough overview, it does reflect the normal Machine Learning workflow:
usually we would train a model and then run tests with the same model on several datasets. However,
with toy.pml we would need to train again everytime we want to test our model on a new test corpus.

Thus this directory also contains three more files: {{{init.pml}}}, {{{train.pml}}} and {{{test.pml}}}.
Calling them in this order will first do some data preprocessing, then training and  finally testing.
This setup allows to do test the same model on different test tests. Moreover, unless the formulae
of the MLN have been changed the init.pml file does not need to be executed when we train
on the same data but change some training parameters.

=Starting thebeast=

You can execute the commands in this tutorial by simply calling
{{{
$ thebeast toy.pml
}}}
or by running thebeast in interactive mode and typing in the statements presented here. For this just start 
{{{
$ thebeast
}}}

All assuming that you {{{cd}}} into {{{examples/align}}} before and that {{{thebeast/bin}}} is in your $PATH$.
 

=Overview=

This tutorial assumes that you already looked at the [SRLTutorial first tutorial] and/or are familar with some of the features of {{{thebeast}}}. The structure of the main file, {{{toy.pml}}},  is essentially the same. Instead of discussing this structure again we will focus on the actual MLN that captures our domain knowledge through a set of formulae.

The top level file that describes the alignment MLN is {{{align.pml}}}. It begins with some predicate definitions and goes on to include the files {{{align-global}}} which contains global formulae (relating two or more hidden ground atoms) and local ones (with only one ground atom). 

{{{
/* Predicate definitions */

// The words of the source string
predicate src_word: Int x SourceWord;

// The words of the target string
predicate tgt_word: Int x TargetWord;

// the true alignments: align(src,tgt) means that token src in the source is aligned
// to token tgt in the target string
predicate align: Int x Int;

// Word to word translation probabilities from IBM model 4
// Note that in practice it might make more sense to provide these probabilties
// one a instance-by-instance basis. This table could become very large and as is
// thebeast would keep it in memory.
predicate model4: SourceWord x TargetWord x Double;

/* Loading the MLN formulae (local and global ones) */
include "align-global.pml";
include "align-local.pml";

/* Defining which predicates are hidden, observed and global. Do not forget this! */
observed: src_word, tgt_word;
hidden: align;
global: model4;
}}}

= Local Formulae =

{{{
// This formula penalizes or rewards the existence of an alignment between a token pair
weight w_bias: Double;
factor: for Int s, Int t
  if src_word(s,_) & tgt_word(t,_) add [align(s,t)] * w_bias;

// This formula checks whether source and target token a specific source word-target
// word combination
weight w_word: SourceWord x TargetWord -> Double;
factor: for Int s, Int t, SourceWord w_s, TargetWord w_t
  if src_word(s,w_s) & tgt_word(t,w_t) add [align(s,t)] * w_word(w_s,w_t);

// This formula is an example for real valued formulae. The formula tests
// whether the tokens of the word have a IBM model 4 translation probability
// and if so add a score scaled by a weight and the probability for this token pair.
weight w_model4: Double;
factor: for Int s, SourceWord w_s, Int t, TargetWord w_t, Double prob
  if src_word(s,w_s) & tgt_word(t,w_t) & model4(w_s,w_t,prob) add [align(s,t)] * prob * w_model4;
}}}

= Global Formulae =
{{{
// (soft) global formula that rewards solutions where words are translated in a
// diagonal fashion
weight w_diag: Double+;
factor: for Int s, Int t 
	if src_word(s,_) & src_word(s+1,_) & tgt_word(t,_) & tgt_word(t+1,_)
	add [align(s,t) => align(s+1,t+1)] * w_diag;
}}}


