#summary One-sentence summary of this page.

= Introduction =

This is a thebeast tutorial. We use a semantic tagger as example application. The task is to label words with semantic slots. For example we want to get a tagging such as

Show me all flights from Hamburg,,FROM_LOC,, to Chicago,,TO_LOC,,

Often a system needs more information than this. We may also be interested in the fact that both Hamburg and Chicago are city names. So in fact we are looking for the following labeling:

Show me all flights from Hamburg,,FROM_LOC&CITY_NAME,, to Chicago,,TO_LOC&CITY_NAME,,
 


= Installation =

== Download ==

Go to "Source" and use svn for now

== compile ==

cd into the thebeast directory and call

{{{
ant -f thebeast.xml
}}}

to compile the source. 

`thebeast` can be started by calling 

{{{
bin/linux/thebeast 
}}}

on linux machines and

{{{
bin/mac/thebeast 
}}}

on macs.

= Defining a model =

== Types ==
{{{
type Slot: From_loc, "to_loc";
type Word: ... "a", "man";
}}}
== Predicates ==
{{{
predicate slot: Int x Slot;
predicate word: Int x Word;
}}}

== Defining the Model ==
{{{
hidden: slot;
observed: word;
}}}

== Local Formulas/Features ==
The weight function (maps different feature instantiations to weights)
{{{
weight w_word: Word x Slot -> Double;
}}}
The formula/factor/feature for the current word
{{{
factor: for Int t, Word w, Slot s if word(t,w) add [slot(t,s)] * w_word(w,s);
}}}

While factors can share weights (untested) we usually have one unique weight function for each factor. If we want to take the next word into account, we need to define a new weight function:
{{{
weight w_word_p1: Word x Slot -> Double;
}}}
and a new formula
{{{
factor: for Int t, Word w, Slot s if word(t+1,w) add [slot(t,s)] * w_word_p1(w,s);
}}}


= Loading Data =

== File Format ==
{{{
>>
>word
0 "from"
1 "NewYork"
2 "to"
3 "Chicago"

>slot
1 From_Loc
1 City_Name
3 To_Loc
3 City_Name

>>
>word
0 "from"
1 "Chicago"
2 "to"
3 "NewYork"

>slot
1 From_Loc
1 City_Name
3 To_Loc
3 City_Name

}}}

the `>>` starts a new database/sentence, the `>predicate-name` a table with true ground atoms for the given predicate.
== Load corpus ==

{{{
load corpus from "corpus1.txt";
}}}

== check data ==

So far we have loaded the corpus, but not into RAM. It will be streamed from files in a sequential manner when we learn or test our model. To inspect the corpus it needs to be loaded into ram by 
{{{
save corpus to ram;
}}} 

Now you can move around the corpus using 
{{{
next;
}}}
and 
{{{
prev
}}}
and 
{{{
print atoms.words;
}}}
to print all words of the current sentence/database.

= Learning =


== Collect Features ==

{{{
collect;
}}}

== Learn ==

{{{
save corpus to instances "/tmp/instances.dmp";
}}}
{{{
set learner.update = "mira";
set learner.loss = "avgF1";
set learner.loss = "globalNumErrors";
}}}

{{{
learn for 10 epochs;
}}}

{{{
save weights to dump "/tmp/weights.dmp";
}}}

= Testing =
{{{
load weights from dump "/tmp/weights.dmp";
load corpus from "/tmp/testcorpus.txt";
test to "/tmp/output.txt";
}}}

== Inspecting the results ==

= Extending the model =






Add your content here.  Format your content with:
  * Text in *bold* or _italic_
  * Headings, paragraphs, and lists
  * Automatic links to other wiki pages