#summary One-sentence summary of this page.

= Introduction =

This is a thebeast tutorial. We use a semantic tagger as example application. The task is to label words with semantic slots. For example we want to get a tagging such as

Show me all flights from Hamburg,,FROM_LOC,, to Chicago,,TO_LOC,,

Often a system needs more information than this. We may also be interested in the fact that both Hamburg and Chicago are city names. So in fact we are looking for the following labeling:

Show me all flights from Hamburg,,FROM_LOC & CITY_NAME,, to Chicago,,TO_LOC & CITY_NAME,,
 
In fact, there can be several slots per token.

Note that using a simple classifier or (single layer) sequential model would be problematic here: it either had to pick always just exactly one slot type or create all possible type combinations and introduce these as new classes. While the first solution will never be able to predict more than one slot type, the second will run into sparsity problems. Finally, we could define a set of binary classifiers, one for each type, but this modelling would not be able to exploit the fact that there are dependencies between the slot types. 

In PML this task can be easily modeled using a predicate `slot` that relates each token with a set of slot types. Using formulas we can then also define dependencies between slot types of the same token or even different tokens.
 
= Installation =

== Download ==

Go to "Source" and use svn for now

== compile ==

cd into the thebeast directory and call

{{{
ant -f thebeast.xml
}}}

to compile the source. 

`thebeast` can be started by calling 

{{{
bin/linux/thebeast 
}}}

on linux machines and

{{{
bin/mac/thebeast 
}}}

on macs.

= Defining the Signature =

First we need to define the signature of our model. A signature consists of a set of types and predicates over these types. 

== Types ==

To define a type we write the following:
{{{
type Slot: From_loc, To_loc;
type Word: ... "a", "man";
}}}
The first row shows a type (the Slot type) that uses capitalized constants (`From_loc`). Note that `from_loc` (with lower case intial"f") will result in a syntax error, because lowercased strings are reserved for predicates and functions.

The second row shows the Word type. It uses string literals in quotes in order to represent lower case words. Also not the `...`! This is a keyword that indicates that the type is "open-ended"; that is, we might encounter strings not defined here. For these we won't be able to learn any parameters, however, we also won't generate an error message if we see them.

== Predicates ==
Based on these types we are now ready to define some predicates. In our example we define the slot and the word as follows:

{{{
predicate slot: Int x Slot;
predicate word: Int x Word;
}}}

Note that `Int` represents a built-in integer type.

= Defining the Model =

First we define which predicates are query or hidden predicates and which predicates are observed. In this task the only hidden predicate is `slot`, `word` is a observed predicate. We define this fact via

{{{
hidden: slot;
observed: word;
}}}

note that you can define more than one hidden or observed predicate by simply enumerating them in a comma-separated list
{{{
observed: word, pos;
}}}

== Local Formulas/Features ==
The weight function (maps different feature instantiations to weights)
{{{
weight w_word: Word x Slot -> Double;
}}}
The formula/factor/feature for the current word
{{{
factor: for Int t, Word w, Slot s if word(t,w) add [slot(t,s)] * w_word(w,s);
}}}

While factors can share weights (untested) we usually have one unique weight function for each factor. If we want to take the next word into account, we need to define a new weight function:
{{{
weight w_word_p1: Word x Slot -> Double;
}}}
and a new formula
{{{
factor: for Int t, Word w, Slot s if word(t+1,w) add [slot(t,s)] * w_word_p1(w,s);
}}}


= Loading Data =

== File Format ==
{{{
>>
>word
0 "from"
1 "NewYork"
2 "to"
3 "Chicago"

>slot
1 From_Loc
1 City_Name
3 To_Loc
3 City_Name

>>
>word
0 "from"
1 "Chicago"
2 "to"
3 "NewYork"

>slot
1 From_Loc
1 City_Name
3 To_Loc
3 City_Name

}}}

the `>>` starts a new database/sentence, the `>predicate-name` a table with true ground atoms for the given predicate.
== Load corpus ==

{{{
load corpus from "corpus1.txt";
}}}

== check data ==

So far we have loaded the corpus, but not into RAM. It will be streamed from files in a sequential manner when we learn or test our model. To inspect the corpus it needs to be loaded into ram by 
{{{
save corpus to ram;
}}} 

Now you can move around the corpus using 
{{{
next;
}}}
and 
{{{
prev
}}}
and 
{{{
print atoms.words;
}}}
to print all words of the current sentence/database.

= Learning =


== Collect Features ==

{{{
collect;
}}}

== Learn ==

{{{
save corpus to instances "/tmp/instances.dmp";
}}}
{{{
set learner.update = "mira";
set learner.loss = "avgF1";
set learner.loss = "globalNumErrors";
}}}

{{{
learn for 10 epochs;
}}}

{{{
save weights to dump "/tmp/weights.dmp";
}}}

= Testing =
{{{
load weights from dump "/tmp/weights.dmp";
load corpus from "/tmp/testcorpus.txt";
test to "/tmp/output.txt";
}}}

== Inspecting the results ==

= Extending the model =






Add your content here.  Format your content with:
  * Text in *bold* or _italic_
  * Headings, paragraphs, and lists
  * Automatic links to other wiki pages