#summary One-sentence summary of this page.

= Introduction =

This is a thebeast tutorial. We use a semantic tagger as example application. The task is to label words with semantic slots. For example we want to get a tagging such as

Show me all flights from Hamburg,,FROM_LOC,, to Chicago,,TO_LOC,,

Often a system needs more information than this. We may also be interested in the fact that both Hamburg and Chicago are city names. So in fact we are looking for the following labeling:

Show me all flights from Hamburg,,FROM_LOC & CITY_NAME,, to Chicago,,TO_LOC & CITY_NAME,,
 
In fact, there can be several slots per token.

Note that using a simple classifier or (single layer) sequential model would be problematic here: it either had to pick always just exactly one slot type or create all possible type combinations and introduce these as new classes. While the first solution will never be able to predict more than one slot type, the second will run into sparsity problems. Finally, we could define a set of binary classifiers, one for each type, but this modelling would not be able to exploit the fact that there are dependencies between the slot types. 

In PML this task can be easily modeled using a predicate `slot` that relates each token with a set of slot types. Using formulas we can then also define dependencies between slot types of the same token or even different tokens.
 
= Installation =

== Download ==

Go to "Source" and use svn for now

== compile ==

cd into the thebeast directory and call

{{{
ant -f thebeast.xml
}}}

to compile the source. 

`thebeast` can be started by calling 

{{{
bin/linux/thebeast 
}}}

on linux machines and

{{{
bin/mac/thebeast 
}}}

on macs.

= Defining the Signature =

First we need to define the signature of our model. A signature consists of a set of types and predicates over these types. 

== Types ==

To define a type we write the following:
{{{
type Slot: From_loc, To_loc;
type Word: ... "a", "man";
}}}
The first row shows a type (the Slot type) that uses capitalized constants (`From_loc`). Note that `from_loc` (with lower case intial"f") will result in a syntax error, because lowercased strings are reserved for predicates and functions.

The second row shows the Word type. It uses string literals in quotes in order to represent lower case words. Also not the `...`! This is a keyword that indicates that the type is "open-ended"; that is, we might encounter strings not defined here. For these we won't be able to learn any parameters, however, we also won't generate an error message if we see them.

== Predicates ==
Based on these types we are now ready to define some predicates. In our example we define the slot and the word as follows:

{{{
predicate slot: Int x Slot;
predicate word: Int x Word;
}}}

Note that `Int` represents a built-in integer type.

= Defining the Model =

First we define which predicates are query or hidden predicates and which predicates are observed. In this task the only hidden predicate is `slot`, `word` is a observed predicate. We define this fact via

{{{
hidden: slot;
observed: word;
}}}

note that you can define more than one hidden or observed predicate by simply enumerating them in a comma-separated list
{{{
observed: word, pos;
}}}

== Local Formulas/Features ==
The weight function (maps different feature instantiations to weights)
{{{
weight w_word: Word x Slot -> Double;
}}}
The formula/factor/feature for the current word
{{{
factor: for Int t, Word w, Slot s if word(t,w) add [slot(t,s)] * w_word(w,s);
}}}
Basically this does what it says: for each token `t`, word `w` and slot type `s` for which `w` is the word at `t` we add a score `w_word(w,s)` if the atom `slot(t,s)` is true, and 0 otherwise. This is a local formula because the delta-function represented by the brackets `[` and `]` only contains one hidden decision to make: `slot(t,s)`. Understanding this format becomes easier if we read the bracket as follows
{{{

[ formula ] = 1 if formula holds
              0 otherwise.

}}}

While factors can share weights (untested) we usually have one unique weight function for each factor. If we want to take the next word into account, we need to define a new weight function:
{{{
weight w_word_p1: Word x Slot -> Double;
}}}
and a new formula
{{{
factor: for Int t, Word w, Slot s if word(t+1,w) & word(t,_) add [slot(t,s)] * w_word_p1(w,s);
}}}
Note the `_`: this indicates a _don't care_ variable. We only want to make sure that there is a word at `t`, we don't care what this word is, thus we write `word(t,_)`.


= Loading Data =

== File Format ==

First, you need to define a file with the examples for the task. The following is an example for a the semantic tagging task.

{{{
>>
>word
0 "from"
1 "NewYork"
2 "to"
3 "Chicago"

>slot
1 From_Loc
1 City_Name
3 To_Loc
3 City_Name

>>
>word
0 "from"
1 "Chicago"
2 "to"
3 "NewYork"

>slot
1 From_Loc
1 City_Name
3 To_Loc
3 City_Name

}}}

the `>>` starts a new database/sentence, the `>predicate-name` a table with true ground atoms for the given predicate.

== Load corpus ==

Once defined the file (e.g. corpus1.txt), you can load it into the beast with:

{{{
load corpus from "corpus1.txt";
}}}

== check data ==

So far we have loaded the corpus, but not into RAM. It will be streamed from files in a sequential manner when we learn or test our model. To inspect the corpus it needs to be loaded into ram by 
{{{
save corpus to ram;
}}} 

Now you can move around the corpus using 
{{{
next;
}}}
and 
{{{
prev;
}}}
and 
{{{
print atoms.words;
}}}
to print all words of the current sentence/database.

= Learning =

For learning we follow the next sequence of actions:
  * Define the signature
  * Define the model
  * Load corpus
  * Collect features
  * Setting learning parameters
  * Begin learing

== Collect Features ==

Before training it is necesary to collect all instances of features from the corpus, this is done with:
{{{
collect;
}}}
To avoid this stage every training time, it's possible to save the instances of the features into a file with:

{{{
save corpus to instances "/tmp/instances.dmp";
}}}

Next time, we wish to train and the model hasn't change we can load this instances instead of collecting them.
{{{
load corpus from instances "/tmp/instances.dmp";
}}}

== Setting parameters for learning ==

There are several parameters to set-up for the learner. We can set them using the command `set`. For instance:
{{{
set learner.update = "mira";
}}}
The `learner` has a `solver` object which has another set of parameters. These are set with:
{{{
set learner.solver.integer = true;
}}}
TODO: Put reference with list of parameters

== Learning process ==

To begin the learing process use:

{{{
learn for 10 epochs;
}}}
where `10` can be any other number of epochs.

Once the learning epochs are finilazed, save the weights into a file (e.g. weights.dmp).
{{{
save weights to dump "/tmp/weights.dmp";
}}}

Note: The beast saves the weights for each epoch unter the `/tmp` directory whith the names 'epoch_n.dmp' where n is the number of the epoch. These files are good to find the whch number of epochs produce the best model.

= Testing =

For testing we follow the next sequence of actions:
  * Define the signature (similar to learning)
  * Define the model (similar to learging)
  * Load weights
  * Load corpus (similar to learning)
  * Setting solver parameters
  * Begin testing process (similar to learning)

== Loading weights ==

To load the weights we use the following command:
{{{
load weights from dump "/tmp/weights.dmp";
}}}

== Setting parameters for the solver ==

There are several parameters to set-up for the solver (notice that the learner has its own solver which parameters are independent of this). We can set them using the command `set`. For instance:
{{{
set solver.integer = true;
}}}
TODO: Put reference with list of parameters

== Testing process ==

To begin the testing procedure (i.e., tagging) we use the command:
{{{
test to "/tmp/output.txt";
}}}

== Inspecting the results ==

Once the testing is finish, it's possible to inspect the results, for this we save the corpus into ram, as previously done:
{{{
save corpus to ram;
}}}
And, we use the triplet of commands:
{{{
 next; solve; print eval;
}}}
We print the atoms to identify the errors:
{{{
 print atoms;
}}}

== Using scripts ==

It is possible to define a sequence of commands into a script file and then loaded into the beast with the `include` command. This is useful to define the signature and the model files and the training, testing, and inspectig sequences we have seen here. An example of the command is:
{{{
include "model.plm";
}}}

= Extending the model =

== Collect all ==

== Global formulae ===

== Hard constrains ===

== Inspecting the features ===
{{{
 print solver.features.slot(6,'NONE');
}}}


Add your content here.  Format your content with:
  * Text in *bold* or _italic_
  * Headings, paragraphs, and lists
  * Automatic links to other wiki pages