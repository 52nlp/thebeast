#LyX 1.6.4.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\prob}{p}
\end_inset


\begin_inset FormulaMacro
\newcommand{\Yvar}{\mathbf{Y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\boldc}{\mathbf{c}}
\end_inset


\end_layout

\begin_layout Title
Growing a Probabilistic Programming Language: Turning Markov Logic into
 a Bazaar
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Abstract
[write a short descriptions of bazaar type prob languages, how we implement
 such a bazaar, and how this helped us to improve compactness and efficiency]
\end_layout

\begin_layout Abstract
At the same time, it remains in flux: new language concepts are constantly
 introduced (blocking, weight constraints, cardinality).
 Many of them are actually not increasing the actual expressivity but make
 inference easier.
 
\end_layout

\begin_layout Abstract
This suggests a language/framework that accomodates for new extensions or
 subsets to the language while re-using a maximal amount of existing infrastruct
ure.
 
\end_layout

\begin_layout Abstract
We therefore propose 
\noun on
Extendable Markov Logic
\noun default
, a language that centers around variables, possible worlds and the notion
 of scoring or prob functions over possible worlds.
 In Ex Markov Logic the developer gets to compose these prob functions.
 The core ML building blocks are quantifications, boolean connectives and
 indicator functions that map boolean values to real values.
 These allow to reproduce Markov Logic in a very intuitive fashion.
 
\end_layout

\begin_layout Abstract
However, the core feature of ExML is that it allows us to readily define
 new building blocks of scoring functions.
 This can be used to directly implement blocking etc.
 It can also be used to introduce novel blocks, such as cardinality constraints,
 or acyclicity constraints.
 
\end_layout

\end_inset

Abstract here.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In Programming, and Programming Language Design, we often speak of two competing
 metaphors: the cathedral and the bazaar
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "raymond99cathedral,steele99growing"

\end_inset

.
 The former is a near-perfect building that, designed by only a handful
 of architects, takes a long time to built and stays unchanged forever.
 The latter is constantly evolving and extended by its users.
 In recent years software development practices as well as language design
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "odersky08programming"

\end_inset

 has shifted attention to the Bazaar---primarily because it allows a program
 or language to grow with the ever changing needs of its users instead of
 trying to anticipate these in advance.
 (This has lead to very successful projects such as Linux, firefox etc.)
\end_layout

\begin_layout Standard
In this paper we argue that Markov Logic
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "richardson06markov"

\end_inset

, to date one of the most widely applied probabilistic programming languages,
 is inherintely a cathedral, and should be made a Bazaar.
 We see it as a cathedral because it only provides very limited possibilities
 for users to extend the language---users can provide static functions such
 as 
\begin_inset Quotes eld
\end_inset

string-edit distance
\begin_inset Quotes erd
\end_inset

 but cannot extend the set of primitive types of formulae
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Note that while it has been evolving (addition of the plus notation, exclamation
 marks, blocking) and extended (to Hybrid Markov Logic Networks, or Decision
 Process MLNs) this evolution has either been quite adhoc, or required to
 change syntax and semantics the language.
 To remain in the above methaphor: the cathedral 
\end_layout

\end_inset

.
 And it should be a Bazaar: a language whose very essence is to grow, change
 and which allows this growth to be triggered and controlled by its users,
 as opposed to a small set of language designer.
\end_layout

\begin_layout Standard
Let us become more concrete.
 Assume a Natural Language Processing (NLP) researcher that designs a joint
 pos tagging and dependency parsing model within Markov Logic.
 She represents the words with a binary 
\family typewriter
word(Token,Word)
\family default
 predicate and starts to design a simple pos tager by introducing a predicate
 
\family typewriter
pos(Token,Pos)
\family default
, and a well-designed set of formulae that describe the rough relation between
 words, their neighborhoods, and their PoS tags.
 For example:
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Plus notation:
\end_layout

\end_inset


\end_layout

\begin_layout LyX-Code

\size footnotesize
word(t,+w) => pos(t,+p)
\end_layout

\begin_layout Standard
She goes on to design dependency parse tree model over the tokens of a sentence
 (see figure).
 She chooses a dependency(Token,Token) predicate to represent such a tree.
 She would add formulae that relate pos tags and words to dependency predicates.
 Moreover, in order to enforce dependency to be a spanning tree she needs
 to both introduce an additional predicate and write down a range of formulae:
\end_layout

\begin_layout LyX-Code

\size footnotesize
dependency(h1,m) ^ dependency(h2,m) => h1=h2
\begin_inset Foot
status open

\begin_layout Plain Layout

\family typewriter
\size footnotesize
//to enforce unique heads
\end_layout

\end_inset

 
\end_layout

\begin_layout LyX-Code

\size footnotesize
m > 0 => exists h: dependency(h,m)
\begin_inset Foot
status open

\begin_layout Plain Layout

\family typewriter
\size footnotesize
every non-root has a head
\end_layout

\end_inset

 
\end_layout

\begin_layout LyX-Code

\size footnotesize
dependency(h,m) => ancestor(h,m)
\begin_inset Foot
status open

\begin_layout Plain Layout

\family typewriter
\size footnotesize
head is an ancestor
\end_layout

\end_inset

 
\end_layout

\begin_layout LyX-Code

\size footnotesize
dependency(h,m) ^ ancestor(a,h) => ancestor(a,m)
\begin_inset Foot
status open

\begin_layout Plain Layout

\family typewriter
\size footnotesize
heads of ancestors are ancestors
\end_layout

\end_inset


\end_layout

\begin_layout LyX-Code

\size footnotesize
ancestor(a,c) => !ancestor(c,a)
\begin_inset Foot
status open

\begin_layout Plain Layout

\family typewriter
\size footnotesize
no loops in the graph
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that while there are other ways to represent the Spanning tree constraint[c
ite cussens], they all require an equal amount of formulae and the addition
 of an auxiliarly predicate.
 
\end_layout

\begin_layout Standard
In this scenario we see three problems (ordered by increasing severness)
 with the Markov Logic cathedral:
\end_layout

\begin_layout Standard
First, Spanning trees appear in several AI applications, such as vision
 (for object graphs), clustering (for hierarchical clusters) and many more.
 Each researcher that needs these spanning tree constraints need to cwrite
 them up himself.
 This leads to
\series bold
 more bugs, more clutter
\series default
 in models.
 If 
\begin_inset Quotes eld
\end_inset

spanning tree
\begin_inset Quotes erd
\end_inset

 was a language primitive (
\family typewriter
spanningTree(dependency)
\family default
), this problem would disappear.
 However, it seems surely wrong to add primitives for each of such cases
 and present a new Language.
 
\end_layout

\begin_layout Standard
Second, after our researcher has carefully designed this model and tested
 it, other researchers want to reuse this model in their own Information
 Extraction model as a black box.
 In this case they need to copy all formulae of this model, and do a copy
 and replace of any predicates that may have different names.
 Programming Languages tell us that this type of 
\series bold
code duplication
\series default
 is bad.
 If 
\begin_inset Quotes eld
\end_inset

tagAndDep
\begin_inset Quotes erd
\end_inset

 could become a primitive of Markov Logic this problem disappears.
\end_layout

\begin_layout Standard
Finally, and most crucially, the model is 
\series bold
not tractable
\series default
 with current Markov Logic inference methods (see experiments).
 However, if we had a spanning tree language construct we could provide
 specialized inference methods for this model substructure
\begin_inset space ~
\end_inset

(cite David, ) and use them in a composible inference method such as BP.
 
\end_layout

\begin_layout Standard
In the following we show how we generalize Markov Logic to become more of
 a Bazaar, and less of Cathedral.
 In essence, we want it to be easy to introduce constructs such as spanningTree(
dependency) and jointTagger(word,pos,dependency), and provide optimized
 inference methods for such constructs that can be seamlessly combined with
 other constructs of the language.
 We do so by a) using a Functional/Higher Order Logic/Simple Type Theory
 semantics, (b) using compositional inference methods that can treat factors
 as black boxes, and (b) using Scala as glue and platform (this allows users
 to.
 We demonstrate the effectiveness of our approach by comparing DP in Markov
 Logic with DP in ExML.
 
\end_layout

\begin_layout Standard
Note that languages such as Church
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "goodman08church"

\end_inset

 or Blog are naturally Bazaars because due to their functional design.
 However, they are generative, and use expression to denote generative models.
 Our work follows the spirit of Markov Logic and defines factor graphs (scoring
 functions over possible worlds).
 Also, they don't focus on more efficient inference for special made constructs.
 Our language is also close in spirit to Factorie, but inherently declarative.
 Also related to Learning-Based Java.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
[go on to build full NLP system, then show problems: other people want to
 use spanning trees (small problem), other people want to use NLP model
 (bigger problem: replace all predicates), model not tractable (biggest
 problem) ]
\end_layout

\begin_layout Plain Layout
Now there are at least two problems with this:
\end_layout

\begin_layout Itemize
This model is not tractable in Markov Logic (see experiments)---it creates
 ground networks that are hard to solve with any of the existing MLN approaches.
 
\end_layout

\begin_layout Itemize
Everyone who needs a spanning tree needs to reproduce these formulae (such
 as vision researchers, hierarchical classification, etc.) in one way of
 another.
 This leads to clutter and more complicated model files (weak).
\end_layout

\begin_layout Plain Layout
Now assume someone in vision who is developing a object reconstruction model
 with MLN.
 He too follows a spanning tree approach to represent the hierarchical object
 graph.
 Hence part of his model looks like what we have seen above---they only
 difference being a predicate 
\begin_inset Quotes eld
\end_inset

parent
\begin_inset Quotes erd
\end_inset

 used instead of dependency.
 In a cathedral, and in Markov Logic, he would have to write the same set
 of formulae again, along with the addition of extra predicates.
 And the same would hold for someone in hierarchical clustering, etc.
 pp.
 In a bazaar, he would simply write 
\end_layout

\begin_layout LyX-Code
spanningTree(parent)
\end_layout

\begin_layout Plain Layout
because someone (the NLP researcher?) provided the primitve (or primitive-like)
 
\family typewriter
spanningTree
\family default
 before.
 [Mention 
\end_layout

\begin_layout Plain Layout
Now why is good? On the surface, we have just reduced 5 formulae to one.
 
\end_layout

\begin_layout Itemize
more compact models, reuse of models in other contexts (factor out dependency
 parser, tagger etc.)
\end_layout

\begin_layout Itemize
provide useful/sensible/intuitive/NATURAL units of optimization 
\end_layout

\begin_layout Itemize
models capture and focus on the application specific structure, not on those
 parts which are generally found in many models.
\end_layout

\begin_layout Plain Layout
How to turn into Bazaar? We turn ML into an Bazaar by (a) generalizing to
 Simple Type Theory/Higher order logic, focussing on inference and learning
 which interact with new language constructs in a black box fashion, and
 (c) embedding it into a growable-Bazaar like deterministic programming
 language (Scala).
\end_layout

\begin_layout Plain Layout
[Markov Logic is a probabilistic programming language, and to date the most
 widely applied one.
 However, it can be seen as a cathedral]
\end_layout

\begin_layout Plain Layout
In contrast, many probabilistic programming languages can be seen as cathedrals:
 they are often designed once with the (impossible goal) of being (a) sufficient
ly expressive while (b) still allowing efficient inference and learning.
 Later on they are only marginally extended to be a bit more expressive,
 or less expressive and more efficient.
 Languages such as Markov Logic RMNs, Bayesian Logic Programs, Prism can
 be seen as such cathedrals.
 
\end_layout

\begin_layout Plain Layout
[now focus on Markov Logic] We believe that the time is ripe for a bazaar-type
 Markov Logic-like probabilistic programming language that allows users
 to introduce new constructions/expressions.
 This belief stems from the following observations:
\end_layout

\begin_layout Itemize
Many probabilistic models contain identical sub-structure (e.g.
 models with tree constraints, matching constraints, markov chains, tree
 alignments).
 By introducing 
\begin_inset Quotes eld
\end_inset

new words into our language
\begin_inset Quotes erd
\end_inset

 these structures can be re-used and lead to more compact and maintable
 models.
 
\end_layout

\begin_layout Itemize
There often exist specialized inference methods for such substructures that
 can be used in methods such as belief propagation or mean field methods.
 By allowing language users to introduce new constructions and giving them
 the opportunity to provide tailor-made inference routines for these constructio
ns these inference methods can be exploited.
 (This helps the language to become an interface to a much larger set of
 AI technology than ML claims to be)
\end_layout

\begin_layout Itemize
One of the most successful PPL, Markov Logic, has been incrementally extended
 over the years.
 Extensions include blocking constraints, + notation, hybrid networks and
 much more.
 While these extensions have been rather adhoc and not part of the actual
 language but of the alchemy interpreter, it shows that there is a need
 for PPLs used in practice to grow.
 
\end_layout

\begin_layout Plain Layout
Note that languages such as Church and Blog can be seen as Bazaar type languages
 since they are based on Lambda Calculus which naturally support growth
 and new constructs, they do not allow for tailor made inference for new
 constructs.
 Moreover, these languages are generative, and constructs in the language
 represent random variables.
 By contrast, here we present a language in wich terms are used to compose
 (deterministic) scoring functions over possible worlds.
 This is along the lines of Markov Logic, parfactors or XX.
 
\end_layout

\begin_layout Plain Layout
To make this more concrete, let us consider dependency parsing, an application
 in Natural Language Processing.
 Here we are asked to ...
\end_layout

\begin_layout Plain Layout
Markov Logic has been successfully applied for various tasks, and is currently
 gaining momentum as one of the leading probababilistic programming languages.
 This is probably due to various reasons: its expressivity, its simplicity,
 the active development of an interpreter, etc.
 However, when looking at the history of Markov Logic, as well as its main
 software manifestiation, alchemy, another reason becomes apparent.
 Markov Logic is actually a somewhat fluent, evolving language that has
 been accomodating new constructs as see fit.
 For example, the plus-notation, blocking syntax, hybrid markov logic, etc.
 
\end_layout

\begin_layout Plain Layout
We argue that this flexibility, or evolution, is essential for the further
 progress of Markov Logic as interface language between applications and
 AI technology.
 In particular, we think that Markov Logic will need to accomodate more
 types of constraints and expressions than predicates, conjunctions and
 quantifications.
 This is best exemplified in the need of tree constraint expressions.
 While it is possible to use FOL to define a certain predicate to be a tree,
 it generally leads to hard inference problems.
 However, there exists a multitude of methods for inference with tree constraint
s.
 By default ML, as interface to AI technology, discards all this methods
 and sees everything as clauses.
 Another example are CPTs: while a MLN can represent each finite distribution,
 it discards the structure both for learning and testing.
 
\end_layout

\begin_layout Plain Layout
In this work we therefore present a framework that will allow Markov Logic
 to grow in the above presented ways: 
\noun on
Extendable Markov Logic
\noun default
 (ExML).
 This is done by extracting what we think are the essential aspects of Markov
 Logic: the declarative construction/assembly of scoring functions for possible
 worlds based on quantified formulae and real valued terms over structured
 first order variables.
 We provide default building blocks for this assembly, but more importantly,
 we provide interfaces and inference frameworks that allow users to add
 new types of building blocks, such as a tree constraint.
 
\end_layout

\begin_layout Plain Layout
ExML is also closely related to factorie: here users can define factor graphs
 (our scoring functions in our lingo) and inference/learning in these graphs
 imperatively.
 This enables them to realize something like a tree constraint by allowing
 the user to plug in jump functions that preserve treeness.
 In a way, ExML can be seen as the declarative pedant to factorie: scoring
 functions are still declaratively composed, but the user can introduce
 new building blocks in this construction.
 
\end_layout

\begin_layout Plain Layout
Roughly speaking, ExML allows you to write
\end_layout

\begin_layout LyX-Code

\size footnotesize
sum {(i,j) => $(pos(i,
\begin_inset Quotes erd
\end_inset

NN
\begin_inset Quotes erd
\end_inset

) & pos(j,
\begin_inset Quotes erd
\end_inset

DT
\begin_inset Quotes erd
\end_inset

) ==> link(i,j)) * 2.5} + tree(link)
\end_layout

\begin_layout Plain Layout
the first summand is essentially the log linear score of a Markov Logic
 formula (note that $ is our shortform for the indicator function mapping
 bools to doubles).
 The second a new building block we defined in our framework.
 It evaluates to 0 if the predicate link is a tree in a given possible world,
 and 
\begin_inset Formula $-\infty$
\end_inset

 otherwise.
 Note that this scoring function is equivalent to 
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \[
\sum_{i,j}\left[pos\left(i,NN\right)\wedge pos\left(j,DT\right)\Rightarrow link\left(i,j\right)\right]\cdot2.5+tree\left(link\right)\]

\end_inset


\end_layout

\begin_layout Plain Layout
Here 
\begin_inset Formula $\left[\cdot\right]$
\end_inset

 is the Iverson bracket[cite].
\end_layout

\begin_layout Plain Layout
The first embodiement of ExML is implemented in Scala.
 This allows us to avoid designing and own syntax for our language---all
 expressions are realized through operator overloading and other techiques
 within Scala.
 It also allows tight integration and the possibility to generate templates.
 It also makes the definition of new building blocks hassle-free.
\end_layout

\begin_layout Plain Layout
Note that ExML also allows variables to be a) propositional, b) functions
 (over functions) and c) categorical values.
 Again, while we can use ML to simulate such variables, our framework is
 cleaner, leads to better inference (or easier to find tractable substructure).
 
\end_layout

\begin_layout Itemize
relation to factorie
\end_layout

\end_inset


\end_layout

\begin_layout Section
Markov Logic
\end_layout

\begin_layout Standard
Our proposed approach to probabilistic programming is based on, or rather,
 inspired by Markov Logic---a combination of first order logic and Markov
 Netoworks.
 It can be seen as a formalism that extends First Order Logic to allow formulae
 that can be violated with some penalty.
 From an alternative point of view, it is an expressive template language
 that uses First Order Logic formulae to instantiate Markov Networks of
 repetitive structure.
 
\end_layout

\begin_layout Standard
[needs fixing] Let us introduce Markov Logic by considering dependency parsing.
 In Markov Logic we can model this task by first introducing a set of logical
 predicates such as 
\emph on
word(Token,Word),
\emph default
 
\emph on
pos(Token,Pos)
\emph default
 and 
\emph on
dependency(Token,Token)
\emph default
.
 Then we specify a set of weighted first order formulae that define a distributi
on over sets of ground atoms of these predicates (or so-called 
\emph on
possible worlds
\emph default
).
\end_layout

\begin_layout Standard
In Markov Logic a set 
\begin_inset Formula $M=\left\{ \left(\phi,w_{\phi}\right)\right\} _{\phi}$
\end_inset

 of weighted first order formulae is called a 
\emph on
Markov Logic Network
\emph default

\begin_inset space ~
\end_inset

(MLN).
 It assigns the probability
\begin_inset Formula \begin{equation}
\prob\left(\y\right)=\frac{1}{Z}\exp\left(\sum_{\left(\phi,w\right)\in M}\sum_{\boldc\in C^{\phi}}\left[\phi\left(\boldc\right)\right]\cdot w\right)\label{eq:prob}\end{equation}

\end_inset

to the possible world 
\begin_inset Formula $\y$
\end_inset

.
 Here 
\begin_inset Formula $C^{\phi}$
\end_inset

 is the set of all possible bindings of the free variables in 
\begin_inset Formula $\phi$
\end_inset

 with the constants of our domain.
 
\begin_inset Formula $\phi\left(\boldc\right)$
\end_inset

 is the 
\emph on
ground formula
\emph default
 we get by replacing the free variables in 
\begin_inset Formula $\phi$
\end_inset

 by the constants in 
\begin_inset Formula $\boldc$
\end_inset

.
 
\begin_inset Formula $\left[\cdot\right]$
\end_inset

 is the Iverson Bracket that returns 1 if the boolean argument inside the
 bracket evaluates to true, and 0 otherwise.
 
\begin_inset Formula $Z$
\end_inset

 is a normalisation constant.
 Note that this distribution corresponds to a Markov Network (the so-called
 
\emph on
Ground Markov Network
\emph default
) where nodes represent ground atoms and factors represent ground formulae.
\end_layout

\begin_layout Standard
By looking at equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prob"

\end_inset

 one can understand Markov Logic as a language that allows us to compose
 probability or scoring functions over possible worlds using a small set
 of building blocks that can be arranged to yield terms conforming to equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prob"

\end_inset

 :
\end_layout

\begin_layout Itemize
Normalization and Exp.
 over a linear product
\end_layout

\begin_layout Itemize
Sum over formula-weight pairs
\end_layout

\begin_layout Itemize
sum over assignments of free variables (quantification)
\end_layout

\begin_layout Itemize
Iverson bracket (mapping booleans to real values)
\end_layout

\begin_layout Itemize
multiplication with weight
\end_layout

\begin_layout Itemize
basic logic connectives and predicates, forall, exists (within Iverson Bracket).
\end_layout

\begin_layout Standard
In other words, a MLN is a term that has the form of equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prob"

\end_inset

.
 [Stress view of MLN as composed scoring function term] That is, we can
 see as our unit of modelling not only as a list of weighted FOL formula,
 but also as a composed scoring function term that follows a set of construction
 rules and has a small set of atomic building blocks.
 Note that in this view the semantics of an MLN are immediately clear---they
 correspond to the 
\begin_inset Quotes eld
\end_inset

natural
\begin_inset Quotes erd
\end_inset

 semantics of the mathematical term in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prob"

\end_inset

.
 While can go ahead and define the semantics of this term even further,
 it should be clear from here.
 Also note that in this view a model developer could essentially directly
 translate his mathematical formulae in his paper into running code.
 
\end_layout

\begin_layout Section
Higher Order Markov Logic
\end_layout

\begin_layout Standard
It should be clear from the section 1 that we want to talk about properties
 not only of simple entities such as tokens or tags, but also about properties
 of graphs (dependency should be a tree) or sets of relations (all token
 properties).
 This clearly suggests a higher order approach
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "farmer08seven"

\end_inset

.
\end_layout

\begin_layout Standard
We want to talk about need terms to 
\end_layout

\begin_layout Standard
Core:
\end_layout

\begin_layout Itemize
Constant, Fun Application, Quantification
\end_layout

\begin_layout Standard
Extended:
\end_layout

\begin_layout Itemize
Indicator, Multiplication, Iverson Bracket (delta/real to boolean), Exp
\end_layout

\begin_layout Section
Some Theory
\end_layout

\begin_layout Itemize
Treat every formula as a term 
\end_layout

\begin_layout Itemize
Functional Type Theory (Church, inspired by Frege) used here
\end_layout

\begin_layout Itemize
MLNs etc: Relational Type Theory
\end_layout

\begin_layout Itemize
Compared to Blog, Church etc: here terms are deterministic, there terms
 are random variables
\end_layout

\begin_layout Itemize
The strength of the lambda-calculus is that it is easily used as a "glue"
 on top of a richer world of primitives.
 Its advantages as a glue are that it has a natural correspondence with
 the way that people program, and natural compilation techniques yield high-perf
ormance code.
 The latter comes through optimizations know as tail-call and continuation-passi
ng, which might be the subject of future talks.
\end_layout

\begin_layout Itemize
The Environment Model of Evaluation (functional)
\end_layout

\begin_layout Itemize
Title: Functional Programming of Scores over Possible Worlds
\end_layout

\begin_deeper
\begin_layout Itemize
vs Function Programming of Random Variables
\end_layout

\end_deeper
\begin_layout Itemize
Formulas are terms.
 Some (http://mally.stanford.edu/Papers/rtt.pdf) argue: In this paper we develop
 an argument that suggests Whitehead & Russell’s approach to the foundations
 of logic, which takes relations as basic, is to be preferred to Frege’s
 and Church’s.(difference between RTTs and FTTs with regard to their ability
 to represent systems containing formulas that aren’t, and can’t be converted
 to, terms.)
\end_layout

\begin_layout Itemize
http://en.wikipedia.org/wiki/Principle_of_compositionality
\end_layout

\begin_layout Itemize
http://en.wikipedia.org/wiki/Value-level_programming
\end_layout

\begin_layout Itemize
http://plato.stanford.edu/entries/type-theory-church/
\end_layout

\begin_layout Itemize
Henkin's (1950) standard model for semantics
\end_layout

\begin_layout Itemize
look at model theory for church type theory and use a bit of their terminology
\end_layout

\begin_layout Itemize
http://imps.mcmaster.ca/doc/seven-virtues.ps
\end_layout

\begin_layout Itemize
Simple Type Theory! valuation function, we are STT with
\end_layout

\begin_deeper
\begin_layout Itemize
many sortedness
\end_layout

\end_deeper
\begin_layout Itemize
Quantification=Definite Description
\end_layout

\begin_layout Itemize
ExML: mixture of Markov Logic, Simple Type Theory (with Many Base types),
 Church/Figaro, Factorie
\end_layout

\begin_layout Itemize
The two semantics of ﬁrst-order logic and stt are based on essentially the
 same ideas: domains of individuals, truth values, and functions; models
 for languages; variable assignments; and valuation functions deﬁned recursively
 on the syntax of expressions.
\end_layout

\begin_layout Section
Variables and Possible Worlds
\end_layout

\begin_layout Standard
In Markov Logic we define probability distributions over possible worlds.
 In the widest sense, a possible world is a mapping from variables to the
 set of values these variables take on.
 In the propositional case, consider a boolean variable 
\begin_inset Formula $x$
\end_inset

, then a possible world is 
\begin_inset Formula $x\rightarrow true$
\end_inset

.
 In the first order case, consider a variable 
\begin_inset Formula $p\in Person\times Person\Rightarrow\mathbb{B}$
\end_inset

.
 Here 
\begin_inset Formula $p$
\end_inset

 is a variable that is mapped to a relation, and a possible world is 
\begin_inset Formula $p\rightarrow\left\{ \left(Anna,Person\right),\ldots\right\} $
\end_inset

.
 
\end_layout

\begin_layout Standard
The first thing we do in ML, and ExML, is to define our basic atomic domains:
 the set of values that our variables can take on.
\end_layout

\begin_layout LyX-Code
val Persons = Domain(
\begin_inset Quotes eld
\end_inset

Anna
\begin_inset Quotes erd
\end_inset

, ...)
\end_layout

\begin_layout LyX-Code
val Ages = Ints(0 until 100)
\end_layout

\begin_layout LyX-Code
val Heights = Doubles(0,230.0)
\end_layout

\begin_layout LyX-Code
...
\end_layout

\begin_layout Standard
Now we define variables.
 In Markov Logic this means predicates, here it can mean anything from propositi
onal variables to complex functional variables:
\end_layout

\begin_layout LyX-Code
val smokes = Var(Persons -> Bools)
\end_layout

\begin_layout LyX-Code
val friends = Var(Persons x Persons -> Bools)
\end_layout

\begin_layout LyX-Code
val height = Var(Person -> Heights)
\end_layout

\begin_layout LyX-Code
...
\end_layout

\begin_layout Standard
A possible world is a assignment from variables to values.
 The common mode of operation is to partially define possible worlds (the
 observations) and infer the rest (or infer marginal probabilities).
 
\end_layout

\begin_layout LyX-Code
val world = new MutableWorld
\end_layout

\begin_layout LyX-Code
world(smokes) = ClosedFunction(Anna->True) //or
\end_layout

\begin_layout LyX-Code
world(smokes(Anna)) = true
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout Standard
Variables can also be functions, and hence predicates:
\end_layout

\begin_layout LyX-Code
val age = 
\begin_inset Quotes eld
\end_inset

age
\begin_inset Quotes erd
\end_inset

 <- Ints(0,120) x Persons -> Bools
\end_layout

\begin_layout Standard
again, a possible world is a mapping from variables to their values.
 In this case
\end_layout

\begin_layout LyX-Code
world(age) = Function((12,
\begin_inset Quotes erd
\end_inset

Anna
\begin_inset Quotes erd
\end_inset

)->true, ...) //or
\end_layout

\begin_layout LyX-Code
world(age(12,
\begin_inset Quotes erd
\end_inset

Anna
\begin_inset Quotes erd
\end_inset

)) = true
\end_layout

\begin_layout Standard
note that functions can also be point to functions.
 
\end_layout

\begin_layout Subsection
Composite Variables
\end_layout

\begin_layout Standard
predicates vs ground atoms
\end_layout

\begin_layout Section
Terms and Scoring Functions
\end_layout

\begin_layout Standard
A Markov Logic Network is a probability or score function defined over possible
 worlds.
 In ExML we follow the same path.
 However, we break the scoring function into building blocks.
 These blocks essentially correspond to the terms we find in the actual
 definition of MLNs, and actually the definitions of scoring functions in
 research papers.
 
\end_layout

\begin_layout Subsection
Terms
\end_layout

\begin_layout Standard
The main unit of these scoring function are terms.
 A term is simply a symbolic value that can be evaluated to a an actual
 value given a possible world.
 For example, a variable is a term.
\end_layout

\begin_layout LyX-Code
smokes
\end_layout

\begin_layout Standard
For a given possible world, this term can be resolved to be, say, 
\begin_inset Formula $\left\{ Anna\rightarrow True\right\} $
\end_inset

, by replacing 
\emph on
smokes
\emph default
 with its value in the possible world.
 The same holds for 
\end_layout

\begin_layout LyX-Code
smokes(Anna)
\end_layout

\begin_layout Standard
Here the term is recursively resolved in the natural way: the constant Anna
 is resolved to the value Anna, the variable smokes to a function from person
 values to booleans, and the function application to the value of the smokes
 function applied to the value Anna.
 For example, this term could evaluate to 
\emph on
True
\emph default
.
 
\end_layout

\begin_layout Standard
How can this be used to define a scoring function? We define scoring functions
 simply as
\emph on
 real valued terms
\emph default
.
 So together with a
\emph on
 Indicator operator
\emph default
 $ that maps booleans to 1.0 or 0.0, this is a scoring function
\end_layout

\begin_layout LyX-Code
$(smokes(Anna))
\end_layout

\begin_layout Standard
which assigns the value 1.0 to worlds where Anna smokes, and 0.0 otherwise.
 Another example is 
\end_layout

\begin_layout LyX-Code
$(smokes(Anna)) + height(Anna)
\end_layout

\begin_layout Standard
Giving the score 181 to the world 
\begin_inset Formula $smokes\rightarrow\left\{ Anna\rightarrow true\right\} ,height\rightarrow\left\{ Anna\rightarrow180\right\} $
\end_inset

.
 Meaning: a world gets more likely with Anna smoking and being tall.
 
\end_layout

\begin_layout Standard
More formally, a term is an instance of an abstract datatype:
\end_layout

\begin_layout Itemize

\emph on
eval
\emph default
 function: maps world to value
\end_layout

\begin_layout Itemize

\emph on
variables
\emph default
 function: returns all atomic free variables inside the term
\end_layout

\begin_layout Subsection
Quantification
\end_layout

\begin_layout Standard
The above presents terms and double/real terms as the building blocks that
 make up scoring functions.
 However, so far we can only talk about specific problem instances.
 We would need to define a new smoking model for each set of persons we
 encounter.
 This is where quantification comes in to play.
 In ExML we can compose scoring functions that apply a certain to ###.
 For example, the scoring function
\end_layout

\begin_layout LyX-Code
$(forall(Persons){x=>smokes(x)})
\end_layout

\begin_layout Standard
assigns 1.0 to a world where everyone smokes and 0 otherwise.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Note that the forall we have here as the usual FOL semantics.
\end_layout

\end_inset

 However, this is not what we want to do.
 Instead we want to assign a higher score to worlds with more people smoking.
\end_layout

\begin_layout LyX-Code
sum(Persons) {x=>$(smokes(x)) * 1.5}
\end_layout

\begin_layout Standard
The scoring function that this term defines explains itself: we create one
 ...
\end_layout

\begin_layout LyX-Code
sum(Persons,Persons) {(x,y)=>$(friends(x,y) ==> (smokes(x)<==>smokes(y))
 * 0.142}
\end_layout

\begin_layout Standard
asd
\end_layout

\begin_layout LyX-Code
forall(Values) = quantified(And,Values)
\end_layout

\begin_layout LyX-Code
exists(Values) = quantified(Or,Values)
\end_layout

\begin_layout LyX-Code
sum(Values) = quantified(Sum,Values)
\end_layout

\begin_layout Standard
and so forth.
 
\end_layout

\begin_layout Standard
In fact we are now ready to specify MLNs, and their propositional counter-parts.
 However, we can already do much more:
\end_layout

\begin_layout LyX-Code
sum(Persons,Ints) {p,a => $(age(p,a)) * $(a) * 2.0}
\end_layout

\begin_layout Subsection
Vectors
\end_layout

\begin_layout Standard
Many probabilistic models are represented as 
\begin_inset Formula \[
exp\left(f\left(\mathbf{x},\mathbf{y}\right)^{\top}\mathbf{w}\right)\]

\end_inset


\end_layout

\begin_layout Standard
Here the moments, parameters of the probabilistic model are clearly separated
 fromt he structural part.
 However, in our current scoring function this is not the case.
 we could write
\end_layout

\begin_layout LyX-Code
vectorsum(Persons,Ints) {p,a => $(age(p,a)) * $(a) * 1_(
\begin_inset Quotes eld
\end_inset

age
\begin_inset Quotes erd
\end_inset

,p)}
\end_layout

\begin_layout Standard
here 1_(p) is the unit vector active at component p.
 Now we have a representation of the feature function.
 
\end_layout

\begin_layout LyX-Code
sum(Persons,Ints) {p,a => $(age(p,a)) * $(a) * w_age(p)}
\end_layout

\begin_layout Standard
but this requires a new variable (w_age) for each formula.
 
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\nexists\ \]

\end_inset


\end_layout

\begin_layout Subsection
Alchemy Style Markov Logic
\end_layout

\begin_layout LyX-Code
$(a(x) & b(x)) * 2.0
\end_layout

\begin_layout LyX-Code
$(a(x)) * 1.0 + $(b(x)) * 1.0
\end_layout

\begin_layout LyX-Code
$$(a(x) & b(x)) * 2.0
\end_layout

\begin_layout LyX-Code
$$(a(x) & b(x)) * 1_(x)
\end_layout

\begin_layout LyX-Code
AlchemyTerm(BooleanTerm) extends Sum(...)
\end_layout

\begin_layout LyX-Code
Individisble(BooleanTerm) extends BooleanTerm
\end_layout

\begin_layout Section
Extending Extentable Markov Logic
\end_layout

\begin_layout Subsection
Trees
\end_layout

\begin_layout LyX-Code
tree(link,word)
\end_layout

\begin_layout Itemize
Network flow representation in ILP
\end_layout

\begin_layout Itemize
Separation algorithm
\end_layout

\begin_layout Itemize
Matrix Tree theorem
\end_layout

\begin_layout Subsection
Similarity Logic
\end_layout

\begin_layout Subsection
Cardinality Constraints
\end_layout

\begin_layout Section
Inference and Learning
\end_layout

\begin_layout Subsection
Trees
\end_layout

\begin_layout LyX-Code
val submodel = x + y + z ...
\end_layout

\begin_layout LyX-Code
submodel.inferMarginalsWith()
\end_layout

\begin_layout Subsection
Hooks
\end_layout

\begin_layout Subsection
Generalized MWS
\end_layout

\begin_layout Subsection
Gradient Based & Online Learning
\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout LyX-Code

\size footnotesize
mymodel = lambda tokenProperties, tag.
 forall p,t,v.
 token(t) & tokenProperty(p) & p(t,+v) =>tag(t,tag) {
\end_layout

\begin_layout LyX-Code

\size footnotesize
  marginalize(...)
\end_layout

\begin_layout LyX-Code

\size footnotesize
}
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/Users/riedelcastro/projects/phd/phdthesis"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
