#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The search for the most likely translation (aka decoding, search, MAP inference)
 given a statistical MT model has traditionally been peformed by greedy
 or beam-based methods/heuristics [site some].
 While being efficient, most of these methods (do we know enough about decoding
 to make any claims here?) have two drawbacks: (1) they are approximate
 and give no bounds as to how far their solution is away from the true optimum--
-this makes it sometimes hard to tell whether the model or the search algorithm
 need to be improved; (2) they don't scale (development-wise, not efficience)
 well with additional global constraints.
 
\end_layout

\begin_layout Standard
By constrast, recently researchers in other NLP appliciation areas have
 increasingly relied on Integer Linear Programming as means to find MAP
 solutions.
 ILP overcomes the two drawbacks mentioned above as is guaranteed to be
 exact, and can be easily used to enforce global constraints through additional
 linear inequalities.
 However, guaranteed exactness, and generic global inference comes usually
 at the price of efficiency.
 
\end_layout

\begin_layout Standard
In fact, ILP has also been used as a method for MT decoding with IBM Model
 4 [cite Germann].
 In [cite] work it became clear that a naive ILP-based does not scale up
 to more than simple short sentences due to an exponential number of constraints
 necessary to represent the decoding problem in ILP.
 However, recent work in Dependency Parsing [cite you and me] showed that
 ILP can still be efficient for very large prograns when used in an incremental
 fashion.
 This raises the question whether incremental/Cutting Plane ILP can also
 be used for decoding of Model 4 for real world sentences.
 
\end_layout

\begin_layout Standard
In this work we show that it is indeed possible to decode Model 4 with increment
al ILP, at least for sentences to up to 30(? we have to be more defensive
 here I think) words, and a simple two-gramm languange model.
 This allowed us to give a definite answer as to how good/bad Model 4 actually
 is, and how good/bad its state-of-the-art rewrite decoder [cite].
 Exact inference increases Bleu score by about 1 point for two language
 pairs when compared to the results of the rewrite decoder [this is not
 really the answer].
 From this we conclude that the rewrite decoder indeed performs well, but
 still can be slightly improved.
 (At the price of efficiency, that is)
\end_layout

\begin_layout Standard
Moreover, we believe that our work can be the basis of exciting and really
 cool and also sexy future work.
 For example, as mentioned ILP allows for a principled and declarative implement
ation of global constraints and hence we may ask whether Model 4 can be
 improved through additional global linguistic constraints; can we map other
 MT problems to an ILP representation, and will this get slower, or maybe
 faster [why could it]? How can larger language models be handled? (such
 models are relatively easy to incorporate in left to right or greedy approaches
 but pose a serious problem in the ILP sense).
 
\end_layout

\begin_layout Standard
Finally, we observed that preprocessing and generating of ILPs is as cpu-intensi
ve as actual inference.
 This leads to the more general question of how to integrate model construction
 more tightly with the ILP solver (maybe by using Pricing strategies).
\end_layout

\begin_layout Section
IBM Model 4
\end_layout

\begin_layout Standard
IBM Model 4 [cite] tells a generative story of how a sequence of (french)
 target word is generated based a sequence of (english) source tokens.
 First, for each english token 
\begin_inset Formula $i$
\end_inset

 we draw a 
\emph on
fertility
\emph default
 
\begin_inset Formula $\phi_{i}$
\end_inset

 from a probability distribution 
\begin_inset Formula $n\left(\phi_{i}|e_{i}\right)$
\end_inset

.
 The fertility 
\begin_inset Formula $\phi_{i}$
\end_inset

 determines the number of french words the english token 
\begin_inset Formula $i$
\end_inset

 is translated to, and we will call words with 
\begin_inset Formula $\phi_{i}>0$
\end_inset

 
\emph on
fertile
\emph default
 and those with 
\begin_inset Formula $\phi_{i}>1$
\end_inset

 
\emph on
very fertile
\emph default
.
 
\end_layout

\begin_layout Standard
Once the fertilities of all tokens are generated, we generate the fertility
 of a invisible english NULL element.
 This element will be translated into french words in the target sentence
 that are not generated by tokens of the English sentence.
 
\end_layout

\begin_layout Standard
After we have determined the fertilities of all source elements (tokens
 and NULL element), we independently draw a sequence of french target words
 
\begin_inset Formula $\left(\tau_{ik}\right)_{k=1\ldots\phi_{i}}$
\end_inset

 for each element 
\begin_inset Formula $i$
\end_inset

 according to a translation table 
\begin_inset Formula $t\left(\tau_{ik}|e_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
In turn we process the english source tokens in sequence in order to pick
 the positions of their generated target words.
 For each fertile english token 
\begin_inset Formula $i$
\end_inset

 we first randomly order all translations of 
\begin_inset Formula $e_{i}$
\end_inset

 into a sequence.
 Then we place the leftmost french word (so called 
\emph on
head
\emph default
) of this sequence at position 
\begin_inset Formula $j$
\end_inset

 based on the averaged (and rounded) position 
\begin_inset Formula $c_{\rho_{i}}$
\end_inset

 of the translations of the last previous fertile word 
\begin_inset Formula $\rho_{i}$
\end_inset

.
 The distance 
\begin_inset Formula $j-c_{\rho_{i}}$
\end_inset

 between 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $c_{\rho_{i}}$
\end_inset

 is picked according to the distribution 
\begin_inset Formula $d_{1}\left(j-c_{\rho_{i}}|class\left(e_{\rho_{i}}\right),class\left(f_{j}\right)\right)$
\end_inset

 conditioned on the word classes
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
todo: Explain
\end_layout

\end_inset

 of the previous fertile english word and the french word we just translated.
 
\end_layout

\begin_layout Standard
For words other than the leftmost (i.e.
 non-head words) the position 
\begin_inset Formula $k$
\end_inset

 is determined with respect to the position of the head word 
\begin_inset Formula $j$
\end_inset

 according to a distribution 
\begin_inset Formula $d_{>1}\left(k-j|class\left(f_{k}\right)\right)$
\end_inset

 .
\end_layout

\begin_layout Standard
Finally, the positions of all Null-generated words are distributed to the
 remaining slots uniformly [explain formula bit]
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
 & p\left(a,f|e\right)=\\
 & \prod_{i=1}^{l}n\left(\phi_{i}|e_{i}\right)\times\prod_{i=1}^{l}\prod_{k=1}^{\phi_{i}}t\left(\tau_{ik},e_{i}\right)\times\\
 & \prod_{i=1,\phi_{i}>0}^{l}d_{1}\left(\pi_{i1}-c_{\rho_{i}}|class\left(e_{\rho_{i}}\right),class\left(\tau_{i1}\right)\right)\times\\
 & \prod_{i=1}^{l}\prod_{k=2}^{\phi_{i}}d_{>1}\left(\pi_{ik}-\pi_{i\left(k-1\right)}|class\left(\tau_{ik}\right)\right)\times\\
 & \left(\begin{array}{c}
m-\phi_{0}\\
\phi_{0}\end{array}\right)p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-2\phi_{0}}\times\\
 & \prod_{k=1}^{\phi_{0}}t\left(\tau_{0k}|\text{NULL}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Integer Linear Programming Formulation
\end_layout

\begin_layout Standard
Following [german etc] we can formulate the [MAP problem, max likelihood
 alignment? There was a proper term for this] for IBM Model 4 as an Integer
 Linear Program.
 To this end we introduce a set of variables.
 
\end_layout

\begin_layout Standard
First, for all pairs of source words (in noisy channel lingo, from here
 on) 
\begin_inset Formula $s_{1}$
\end_inset

 and 
\begin_inset Formula $s_{2}$
\end_inset

 we define a variable 
\begin_inset Formula $f_{s_{1},s_{2}}$
\end_inset

 that yields 1 if the word 
\begin_inset Formula $j$
\end_inset

 follows the word 
\begin_inset Formula $i$
\end_inset

 in the source translation, and zero otherwise.
 Furthermore, for each target word 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $t$
\end_inset

 and each source word 
\begin_inset Formula $s$
\end_inset

 with 
\begin_inset Formula $s\in S\left(t\right)$
\end_inset

 [the set of possible translations for 
\begin_inset Formula $t$
\end_inset

] the variable 
\begin_inset Formula $a_{s}^{t}$
\end_inset

 is 1 if 
\begin_inset Formula $s$
\end_inset

 is the translation of 
\begin_inset Formula $t$
\end_inset

 and 0 otherwise.
 
\end_layout

\begin_layout Standard
To ensure that each assignment of the 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $a$
\end_inset

 variables represents a model 4 translation we add the following constraints.
\end_layout

\begin_layout Description
One
\begin_inset space ~
\end_inset

Active
\begin_inset space ~
\end_inset

Source For all target words 
\begin_inset Formula $t$
\end_inset


\begin_inset Formula \[
\sum_{s\in S\left(t\right)}a_{s}^{t}=1\]

\end_inset


\end_layout

\begin_layout Description
No
\begin_inset space ~
\end_inset

Cycles For all cycles 
\begin_inset Formula $C$
\end_inset

 of source words
\begin_inset Formula \[
\sum_{\left(s_{1},s_{2}\right)\in C}f_{s_{1},s_{2}}\leq1\]

\end_inset


\end_layout

\begin_layout Description
Follows
\begin_inset space ~
\end_inset

Active
\begin_inset space ~
\end_inset

Consistency Hmmm? Not sure how to translate the ML here in the best way.
 Todo
\end_layout

\begin_layout Description
Null
\begin_inset space ~
\end_inset

Issues Blah
\end_layout

\begin_layout Standard
Finally, the linear objective function incorporates the model 4 probabilities
 of a translation (as represented by an assignment for 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $a$
\end_inset

 variables) 
\begin_inset Formula \[
\sum_{t}\sum_{s\in S\left(t\right)}w_{s}^{t}a_{s}^{t}+\sum_{s_{1},s_{2}}w_{s_{1},s_{2}}f_{s_{1},s_{2}}\]

\end_inset

 where 
\begin_inset Formula $w_{s}^{t}$
\end_inset

 is 
\begin_inset Formula $\ldots$
\end_inset

 and 
\begin_inset Formula $w_{s_{1},s_{2}}$
\end_inset


\end_layout

\begin_layout Section
Cutting Plane Algorithm
\end_layout

\begin_layout Standard
The Integer Linear Program we have described above has an exponential number
 of (cycle) constraints.
 Hence, simply passing the ILP to an off-the-shelf ILP solver is not practical
 for all but the smallest sentences.
 For this reason the original 
\begin_inset Quotes eld
\end_inset

Optimal Decoding
\begin_inset Quotes erd
\end_inset

 work only considers sentences up to a length of 8? words.
 However, recent work [riedel&clarke] has shown that even exponentially
 large MAP problems can efficiently solved using ILP solvers if a so-called
 Cutting-Plane Algorithm is used.
 In the following we will present this algorithm in a nutshell.
\end_layout

\begin_layout Algorithm
Cutting Plane algorithm for MT
\end_layout

\begin_deeper
\begin_layout Enumerate
Construct ILP 
\begin_inset Formula $I$
\end_inset

 without cycle constraints
\end_layout

\begin_layout Enumerate

\series bold
do
\end_layout

\begin_deeper
\begin_layout Enumerate
solve 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $I$
\end_inset

 and assign to 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_layout Enumerate
find cycles in 
\begin_inset Formula $ $
\end_inset

solution
\end_layout

\begin_layout Enumerate
add corresponding cycle constraints to 
\begin_inset Formula $I$
\end_inset


\end_layout

\begin_layout Standard

\series bold
until
\series default
 no more cycles can be found
\end_layout

\end_deeper
\begin_layout Enumerate
return 
\begin_inset Formula $y$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
The Cutting Plane algorithm starts with a subset of the complete set of
 constraints, namely all constraints but the (exponentially many) cycle
 constraints.
 The corresponding ILP is solved by a standard ILP solver, and the solution
 
\begin_inset Formula $y$
\end_inset

 is inspected for cycles.
 If it contains no cycles we are done (we have found the true optimum: the
 solution with highest score that does not violate any constraints).
 If the solution does contain cycles, the corresponding constraints are
 added to the ILP which is in turn solved again.
 This process is continued until no more cycles can be found.
 
\end_layout

\begin_layout Standard
It is difficult to make claims about a guaranteed worst-case runtime (or
 number of iterations) of this algorithm.
 However, if the linear scoring function (in other words, the translation
 model and language model parameters) already provides a preference for
 cycle-free solutions, we can expect this algorithm to be efficient.
 For example, if we assume that the translation/distortion model has a very
 strong preference for monotonic solutions then clearly the highest scoring
 solution is likely to be cycle-free.
\end_layout

\begin_layout Section
Rewrite Decoder
\end_layout

\begin_layout Standard
Briefly describe...
\end_layout

\end_body
\end_document
