#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine natbib_authoryear
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The search for the most likely translation (aka decoding, search, MAP inference)
 given a statistical MT model has traditionally been peformed by greedy
 or beam-based methods/heuristics [site some].
 While being efficient, most of these methods (do we know enough about decoding
 to make any claims here?) have two drawbacks: (1) they are approximate
 and give no bounds as to how far their solution is away from the true optimum--
-this makes it sometimes hard to tell whether the model or the search algorithm
 need to be improved; (2) they don't scale (development-wise, not efficience)
 well with additional global constraints.
 
\end_layout

\begin_layout Standard
By constrast, recently researchers in other NLP appliciation areas have
 increasingly relied on Integer Linear Programming as means to find MAP
 solutions.
 ILP overcomes the two drawbacks mentioned above as is guaranteed to be
 exact, and can be easily used to enforce global constraints through additional
 linear inequalities.
 However, guaranteed exactness, and generic global inference comes usually
 at the price of efficiency.
 
\end_layout

\begin_layout Standard
In fact, ILP has also been used as a method for MT decoding with IBM Model
 4 [cite Germann].
 In [cite] work it became clear that a naive ILP-based does not scale up
 to more than simple short sentences due to an exponential number of constraints
 necessary to represent the decoding problem in ILP.
 However, recent work in Dependency Parsing [cite you and me] showed that
 ILP can still be efficient for very large prograns when used in an incremental
 fashion.
 This raises the question whether incremental/Cutting Plane ILP can also
 be used for decoding of Model 4 for real world sentences.
 
\end_layout

\begin_layout Standard
In this work we show that it is indeed possible to decode Model 4 with increment
al ILP, at least for sentences to up to 30(? we have to be more defensive
 here I think) words, and a simple two-gramm languange model.
 This allowed us to give a definite answer as to how good/bad Model 4 actually
 is, and how good/bad its state-of-the-art rewrite decoder [cite].
 Exact inference increases Bleu score by about 1 point for two language
 pairs when compared to the results of the rewrite decoder [this is not
 really the answer].
 From this we conclude that the rewrite decoder indeed performs well, but
 still can be slightly improved.
 (At the price of efficiency, that is)
\end_layout

\begin_layout Standard
Moreover, we believe that our work can be the basis of exciting and really
 cool and also sexy future work.
 For example, as mentioned ILP allows for a principled and declarative implement
ation of global constraints and hence we may ask whether Model 4 can be
 improved through additional global linguistic constraints; can we map other
 MT problems to an ILP representation, and will this get slower, or maybe
 faster [why could it]? How can larger language models be handled? (such
 models are relatively easy to incorporate in left to right or greedy approaches
 but pose a serious problem in the ILP sense).
 
\end_layout

\begin_layout Standard
Finally, we observed that preprocessing and generating of ILPs is as cpu-intensi
ve as actual inference.
 This leads to the more general question of how to integrate model construction
 more tightly with the ILP solver (maybe by using Pricing strategies).
\end_layout

\begin_layout Section
IBM Model 4
\end_layout

\begin_layout Standard
IBM Model 4 [cite] tells a generative story of how a sequence of (french)
 target word is generated based a sequence of (english) source tokens.
 First, for each english token 
\begin_inset Formula $i$
\end_inset

 we draw a 
\emph on
fertility
\emph default
 
\begin_inset Formula $\phi_{i}$
\end_inset

 from a probability distribution 
\begin_inset Formula $n\left(\phi_{i}|e_{i}\right)$
\end_inset

.
 The fertility 
\begin_inset Formula $\phi_{i}$
\end_inset

 determines the number of french words the english token 
\begin_inset Formula $i$
\end_inset

 is translated to, and we will call words with 
\begin_inset Formula $\phi_{i}>0$
\end_inset

 
\emph on
fertile
\emph default
 and those with 
\begin_inset Formula $\phi_{i}>1$
\end_inset

 
\emph on
very fertile
\emph default
.
 
\end_layout

\begin_layout Standard
Once the fertilities of all tokens are generated, we generate the fertility
 of a invisible english NULL element.
 This element will be translated into french words in the target sentence
 that are not generated by tokens of the English sentence.
 
\end_layout

\begin_layout Standard
After we have determined the fertilities of all source elements (tokens
 and NULL element), we independently draw a sequence of french target words
 
\begin_inset Formula $\left(\tau_{ik}\right)_{k=1\ldots\phi_{i}}$
\end_inset

 for each element 
\begin_inset Formula $i$
\end_inset

 according to a translation table 
\begin_inset Formula $t\left(\tau_{ik}|e_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
In turn we process the english source tokens in sequence in order to pick
 the positions of their generated target words.
 For each fertile english token 
\begin_inset Formula $i$
\end_inset

 we first randomly order all translations of 
\begin_inset Formula $e_{i}$
\end_inset

 into a sequence.
 Then we place the leftmost french word (so called 
\emph on
head
\emph default
) of this sequence at position 
\begin_inset Formula $j$
\end_inset

 based on the averaged (and rounded) position 
\begin_inset Formula $c_{\rho_{i}}$
\end_inset

 of the translations of the last previous fertile word 
\begin_inset Formula $\rho_{i}$
\end_inset

.
 The distance 
\begin_inset Formula $j-c_{\rho_{i}}$
\end_inset

 between 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $c_{\rho_{i}}$
\end_inset

 is picked according to the distribution 
\begin_inset Formula $d_{1}\left(j-c_{\rho_{i}}|class\left(e_{\rho_{i}}\right),class\left(f_{j}\right)\right)$
\end_inset

 conditioned on the word classes
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
todo: Explain
\end_layout

\end_inset

 of the previous fertile english word and the french word we just translated.
 
\end_layout

\begin_layout Standard
For words other than the leftmost (i.e.
 non-head words) the position 
\begin_inset Formula $k$
\end_inset

 is determined with respect to the position of the head word 
\begin_inset Formula $j$
\end_inset

 according to a distribution 
\begin_inset Formula $d_{>1}\left(k-j|class\left(f_{k}\right)\right)$
\end_inset

 .
\end_layout

\begin_layout Standard
Finally, the positions of all Null-generated words are distributed to the
 remaining slots uniformly [explain formula bit]
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
 & p\left(a,f|e\right)=\\
 & \prod_{i=1}^{l}n\left(\phi_{i}|e_{i}\right)\times\prod_{i=1}^{l}\prod_{k=1}^{\phi_{i}}t\left(\tau_{ik},e_{i}\right)\times\\
 & \prod_{i=1,\phi_{i}>0}^{l}d_{1}\left(\pi_{i1}-c_{\rho_{i}}|class\left(e_{\rho_{i}}\right),class\left(\tau_{i1}\right)\right)\times\\
 & \prod_{i=1}^{l}\prod_{k=2}^{\phi_{i}}d_{>1}\left(\pi_{ik}-\pi_{i\left(k-1\right)}|class\left(\tau_{ik}\right)\right)\times\\
 & \left(\begin{array}{c}
m-\phi_{0}\\
\phi_{0}\end{array}\right)p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-2\phi_{0}}\times\\
 & \prod_{k=1}^{\phi_{0}}t\left(\tau_{0k}|\text{NULL}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Integer Linear Programming Decoding
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\source}{\mathbf{e}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\target}{\mathbf{f}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\align}{\mathbf{a}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\start}{\text{START}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\stop}{\text{END}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\null}{\text{NULL}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\sourceset}{S}
\end_inset


\end_layout

\begin_layout Standard
Given a trained IBM model 4, and a French sentence 
\begin_inset Formula $\target$
\end_inset

 we need to find the English sentence 
\begin_inset Formula $\source$
\end_inset

 and alignment 
\begin_inset Formula $\align$
\end_inset

 with maximal 
\begin_inset Formula $p\left(\align,\source|f\right)\backsimeq p\left(\source\right)\cdot p\left(\align,\target|\source\right)$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "germann01fast"

\end_inset

 showed that we can formulate this problem using Integer Linear Programming.
 In this section we will present our variant of their ILP formulation.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Our formulation differs slightly because we use a first order modelling
 language that imposed certain restrictions on the type of constraints allowed.
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "germann01fast"

\end_inset

 framed the search for the highest scoring English sentence and alignment
 as the search for a single path over a set of English candidate tokens.
 Let 
\begin_inset Formula $S_{i,10}$
\end_inset

 be the set of 10 most likely translations of 
\begin_inset Formula $f_{i}$
\end_inset

 according to the probability 
\begin_inset Formula $p\left(e|f_{i}\right)$
\end_inset

.
 Then we define the set of candidate English tokens 
\begin_inset Formula $S$
\end_inset

 as
\begin_inset Formula \[
\left\{ s_{I}^{e}|I\neq\emptyset\wedge\forall i\in I:e\in\sourceset_{i,10}\right\} \cup\left\{ s_{\left\{ 0\right\} }^{\start},s_{\left\{ n+1\right\} }^{\stop}\right\} \]

\end_inset

That is, 
\begin_inset Formula $\sourceset$
\end_inset

 contains one token candidate 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 for each English word 
\begin_inset Formula $e$
\end_inset

 and possible non-empty set 
\begin_inset Formula $I$
\end_inset

 of French tokens that can (likely) generate 
\begin_inset Formula $e$
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Note that this search space restriction is based on a reversal of the noisy
 channel metaphor where source words generate target words.
 This approach is consistent with previous work [German], and significantly
 reduces the noise that would appear in a heuristic based on 
\begin_inset Formula $p\left(f|e\right)$
\end_inset

 instead.
 However, it should be remembered that strictly speaking we (and [German])
 do not perform optimal decoding in IBM Model 4, but in a heuristically
 simplified version.
\end_layout

\end_inset

 In addition, 
\begin_inset Formula $\sourceset$
\end_inset

 contains 
\begin_inset Formula $s_{\left\{ 0\right\} }^{\start}$
\end_inset

 and 
\begin_inset Formula $s_{\left\{ n+1\right\} }^{\stop}$
\end_inset

 tokens that serve as the translation of sentence beginning and end, respectivel
y.
 For example, if the English word 
\begin_inset Quotes eld
\end_inset

is
\begin_inset Quotes erd
\end_inset

 can be generated by 
\begin_inset Formula $f_{1}=\text{CE}$
\end_inset

 and 
\begin_inset Formula $f_{2}=\text{NE}$
\end_inset

 in the sentence 
\begin_inset Quotes eld
\end_inset

CE
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $_{1}$
\end_inset

 NE
\begin_inset Formula $_{2}$
\end_inset

 EST
\begin_inset Formula $_{3}$
\end_inset

 PAS
\begin_inset Formula $_{4}$
\end_inset

 CLAIR
\begin_inset Formula $_{5}$
\end_inset


\begin_inset Quotes erd
\end_inset

, then 
\begin_inset Formula $\sourceset$
\end_inset

 contains, among others, 
\begin_inset Formula $s_{\left\{ 1\right\} }^{\text{is}},s_{\left\{ 2\right\} }^{\text{is}}$
\end_inset

 and 
\begin_inset Formula $s_{\left\{ 1,2\right\} }^{\text{is}}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Each acyclic path 
\begin_inset Formula $\left(s_{\left\{ 0\right\} }^{\start},s_{I_{1}}^{e_{1}},\ldots,s_{I_{m}}^{e_{m}},s_{\left\{ n+1\right\} }^{\stop}\right)$
\end_inset

 defines an English sentence 
\begin_inset Formula $\source=\left(e_{1},\ldots,e_{m}\right)$
\end_inset

 .
 A path also defines an alignment 
\begin_inset Formula $\align$
\end_inset

: a token 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 that appears in a path is aligned to all target tokens 
\begin_inset Formula $i$
\end_inset

 in 
\begin_inset Formula $I$
\end_inset

 (in other words: source token 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 generates all target tokens in 
\begin_inset Formula $I$
\end_inset

 --- say something about noisy channel being used here again, instead of
 reverse).
\end_layout

\begin_layout Standard
We describe a path through the English tokens 
\begin_inset Formula $S$
\end_inset

 using two types of variables.
 First, for each 
\begin_inset Formula $s_{I}^{e}\in\sourceset$
\end_inset

 we have a binary variable 
\begin_inset Formula $\alpha_{I}^{e}$
\end_inset

 that indicates whether token 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 is part of the path.
 Second, we define a binary variable 
\begin_inset Formula $\sigma_{I_{1},I_{1}}^{e_{1},e_{2}}$
\end_inset

 for each 
\begin_inset Formula $s_{I_{1}}^{e_{1}},s_{I_{2}}^{e_{2}}\in S$
\end_inset

 with 
\begin_inset Formula $I_{1}\cap I_{2}=\emptyset$
\end_inset

 that indicates whether token 
\begin_inset Formula $s_{I_{1}}^{e_{1}}$
\end_inset

 is followed by token 
\begin_inset Formula $s_{I_{2}}^{e_{2}}\in S$
\end_inset

 in the English sentence.
\end_layout

\begin_layout Standard
In order for the 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\sigma$
\end_inset

 variables to represent a valid path and alignment we add the following
 constraints to our ILP.
 First, for each French token 
\begin_inset Formula $i$
\end_inset

 there is exactly one English token 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 with 
\begin_inset Formula $i\in I$
\end_inset

 that generates it: 
\begin_inset Formula \[
\sum_{i\in I}\alpha_{I}^{e}=1\]

\end_inset

Second, for each English token candidate 
\begin_inset Formula $\alpha_{I}^{e}$
\end_inset

 we ensure that if it is part of the path, there needs to be exactly one
 English token that follows it and one that precedes it (note that for the
 start and end tokens this constraint has to be altered accordingly).
\begin_inset Formula \[
\alpha_{I}^{e}=\sum_{k}\sigma_{I,J_{k}}^{e,e_{k}}=\sum_{k}\sigma_{J_{k},I}^{e_{k},e}\]

\end_inset

Third, we need to guarantee that the path contains no cycles.
 This can be achieved by an exponential number of constraints, one for each
 cycle 
\begin_inset Formula $C=\left(s_{I_{1}}^{e_{1}},\ldots,,s_{I_{m}=I_{1}}^{e_{m}=e_{1}}\right)$
\end_inset

: 
\begin_inset Formula \[
\sum_{i=1}^{m-1}\sigma_{I_{i},I_{i+1}}^{e_{i},e_{i+1}}\leq1\]

\end_inset

Finally, we follow 
\begin_inset CommandInset citation
LatexCommand citet
key "germann01fast"

\end_inset

 and require that there is at most a single active 
\begin_inset Formula $\null$
\end_inset

 English token 
\begin_inset Formula $s_{I}^{\null}$
\end_inset

 (which generates all spurious French words in 
\begin_inset Formula $I$
\end_inset

), and that this token is not part of the actual path.
 We omit the corresponding constraints for brevity.
\end_layout

\begin_layout Standard
The objective function
\begin_inset Formula \[
\sum w_{I}^{e}\alpha_{I}^{e}+\sum w_{I_{1},I_{1}}^{e_{1},e_{2}}\sigma_{I_{1},I_{1}}^{e_{1},e_{2}}\]

\end_inset

ensures that each path and alignment is assigned the correct log probability
 according to model 4.
 Here each 
\begin_inset Formula $w_{I}^{e}$
\end_inset

 is the sum of all log probabilities corresponding to the decision that
 the tokens 
\begin_inset Formula $I$
\end_inset

 are generated by 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 (i.e., fertility and translation probabilities).
 Likewise, each 
\begin_inset Formula $w_{I_{1},I_{1}}^{e_{1},e_{2}}$
\end_inset

 sums up all log probabilities corresponding to the decision that 
\begin_inset Formula $s_{I_{1}}^{e_{1}}$
\end_inset

 is followed by 
\begin_inset Formula $s_{I_{2}}^{e_{2}}$
\end_inset

 (i.e., distortion and language model probabilities).
\end_layout

\begin_layout Section
Cutting Plane Algorithm
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
\end_inset


\end_layout

\begin_layout Standard
The Integer Linear Program we have described above has an exponential number
 of (cycle) constraints.
 Hence, simply passing the ILP to an off-the-shelf ILP solver is not practical
 for all but the smallest sentences.
 For this reason the original 
\begin_inset Quotes eld
\end_inset

Optimal Decoding
\begin_inset Quotes erd
\end_inset

 work only considers sentences up to a length of 8 words.
 However, recent work
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "riedel06incremental"

\end_inset

 has shown that even exponentially large decoding/search problems can efficientl
y solved using ILP solvers if a so-called Cutting-Plane Algorithm
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "dantzig54solution"

\end_inset

 is used.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
It is worth mentioning that Cutting Plane Algorithms have been successfully
 applied for solving very large instances of the Travelling Salesman Problem,
 a problem essentially equivalent to the decoding in Model 4 [cite someone?.
\end_layout

\end_inset

 In the following we will present a Cutting Plane algorithm for IBM Model
 4:
\end_layout

\begin_layout Enumerate
Construct ILP 
\begin_inset Formula $I$
\end_inset

 without cycle constraints
\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
do
\end_layout

\begin_deeper
\begin_layout Enumerate
solve 
\begin_inset Formula $I$
\end_inset

 and assign to solution 
\begin_inset Formula $\y$
\end_inset


\end_layout

\begin_layout Enumerate
find cyclic paths in solution 
\begin_inset Formula $\y$
\end_inset


\end_layout

\begin_layout Enumerate
add corresponding cycle constraints to 
\begin_inset Formula $I$
\end_inset


\end_layout

\begin_layout Standard

\series bold
until
\series default
 no more cyclic paths can be found
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
return
\series default
 
\begin_inset Formula $\y$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
The Cutting Plane algorithm starts with a subset of the complete set of
 constraints, namely all constraints but the (exponentially many) cycle
 constraints.
 The corresponding ILP is solved by a standard ILP solver, and the solution
 
\begin_inset Formula $y$
\end_inset

 is inspected for cycles.
 If it contains no cycles we are done (we have found the true optimum: the
 solution with highest score that does not violate any constraints).
 If the solution does contain cycles, the corresponding constraints are
 added to the ILP which is in turn solved again.
 This process is continued until no more cycles can be found.
 
\end_layout

\begin_layout Standard
It is difficult to make claims about a guaranteed worst-case runtime (or
 number of iterations) of this algorithm.
 However, if the linear scoring function (in other words, the translation
 model and language model parameters) already provides a preference for
 cycle-free solutions, we can expect this algorithm to be efficient.
 For example, if we assume that the translation/distortion model has a very
 strong preference for monotonic solutions then clearly the highest scoring
 solution is likely to be cycle-free.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "seb"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document
