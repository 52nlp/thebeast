#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The search for the most likely translation (aka decoding, search, MAP inference)
 given a statistical MT model has traditionally been peformed by greedy
 or beam-based methods/heuristics [site some].
 While being efficient, most of these methods (do we know enough about decoding
 to make any claims here?) have two drawbacks: (1) they are approximate
 and give no bounds as to how far their solution is away from the true optimum--
-this makes it sometimes hard to tell whether the model or the search algorithm
 need to be improved; (2) they don't scale (development-wise, not efficience)
 well with additional global constraints.
 
\end_layout

\begin_layout Standard
By constrast, recently researchers in other NLP appliciation areas have
 increasingly relied on Integer Linear Programming as means to find MAP
 solutions.
 ILP overcomes the two drawbacks mentioned above as is guaranteed to be
 exact, and can be easily used to enforce global constraints through additional
 linear inequalities.
 However, guaranteed exactness, and generic global inference comes usually
 at the price of efficiency.
 
\end_layout

\begin_layout Standard
In fact, ILP has also been used as a method for MT decoding with IBM Model
 4 [cite Germann].
 In [cite] work it became clear that a naive ILP-based does not scale up
 to more than simple short sentences due to an exponential number of constraints
 necessary to represent the decoding problem in ILP.
 However, recent work in Dependency Parsing [cite you and me] showed that
 ILP can still be efficient for very large prograns when used in an incremental
 fashion.
 This raises the question whether incremental/Cutting Plane ILP can also
 be used for decoding of Model 4 for real world sentences.
 
\end_layout

\begin_layout Standard
In this work we show that it is indeed possible to decode Model 4 with increment
al ILP, at least for sentences to up to 30(? we have to be more defensive
 here I think) words, and a simple two-gramm languange model.
 This allowed us to give a definite answer as to how good/bad Model 4 actually
 is, and how good/bad its state-of-the-art rewrite decoder [cite].
 Exact inference increases Bleu score by about 1 point for two language
 pairs when compared to the results of the rewrite decoder [this is not
 really the answer].
 From this we conclude that the rewrite decoder indeed performs well, but
 still can be slightly improved.
 (At the price of efficiency, that is)
\end_layout

\begin_layout Standard
Moreover, we believe that our work can be the basis of exciting and really
 cool and also sexy future work.
 For example, as mentioned ILP allows for a principled and declarative implement
ation of global constraints and hence we may ask whether Model 4 can be
 improved through additional global linguistic constraints; can we map other
 MT problems to an ILP representation, and will this get slower, or maybe
 faster [why could it]? How can larger language models be handled? (such
 models are relatively easy to incorporate in left to right or greedy approaches
 but pose a serious problem in the ILP sense).
 
\end_layout

\begin_layout Standard
Finally, we observed that preprocessing and generating of ILPs is as cpu-intensi
ve as actual inference.
 This leads to the more general question of how to integrate model construction
 more tightly with the ILP solver (maybe by using Pricing strategies).
\end_layout

\begin_layout Section
IBM Model 4
\end_layout

\begin_layout Standard
IBM Model 4 [cite] tells a generative story of how a sequence of (french)
 target word is generated based a sequence of (english) source tokens.
 First, for each english token 
\begin_inset Formula $i$
\end_inset

 we draw a 
\emph on
fertility
\emph default
 
\begin_inset Formula $\phi_{i}$
\end_inset

 from a probability distribution 
\begin_inset Formula $n\left(\phi_{i}|e_{i}\right)$
\end_inset

.
 The fertility 
\begin_inset Formula $\phi_{i}$
\end_inset

 determines the number of french words the english token 
\begin_inset Formula $i$
\end_inset

 is translated to, and we will call words with 
\begin_inset Formula $\phi_{i}>0$
\end_inset

 
\emph on
fertile
\emph default
 and those with 
\begin_inset Formula $\phi_{i}>1$
\end_inset

 
\emph on
very fertile
\emph default
.
 
\end_layout

\begin_layout Standard
Once the fertilities of all tokens are generated, we generate the fertility
 of a invisible english NULL element.
 This element will be translated into french words in the target sentence
 that are not generated by tokens of the English sentence.
 
\end_layout

\begin_layout Standard
After we have determined the fertilities of all source elements (tokens
 and NULL element), we independently draw a sequence of french target words
 
\begin_inset Formula $\left(\tau_{ik}\right)_{k=1\ldots\phi_{i}}$
\end_inset

 for each element 
\begin_inset Formula $i$
\end_inset

 according to a translation table 
\begin_inset Formula $t\left(\tau_{ik}|e_{i}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
In turn we process the english source tokens in sequence in order to pick
 the positions of their generated target words.
 For each fertile english token 
\begin_inset Formula $i$
\end_inset

 we first randomly order all translations of 
\begin_inset Formula $e_{i}$
\end_inset

 into a sequence.
 Then we place the leftmost french word (so called 
\emph on
head
\emph default
) of this sequence at position 
\begin_inset Formula $j$
\end_inset

 based on the averaged (and rounded) position 
\begin_inset Formula $c_{\rho_{i}}$
\end_inset

 of the translations of the last previous fertile word 
\begin_inset Formula $\rho_{i}$
\end_inset

.
 The distance 
\begin_inset Formula $j-c_{\rho_{i}}$
\end_inset

 between 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $c_{\rho_{i}}$
\end_inset

 is picked according to the distribution 
\begin_inset Formula $d_{1}\left(j-c_{\rho_{i}}|class\left(e_{\rho_{i}}\right),class\left(f_{j}\right)\right)$
\end_inset

 conditioned on the word classes
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
todo: Explain
\end_layout

\end_inset

 of the previous fertile english word and the french word we just translated.
 
\end_layout

\begin_layout Standard
For words other than the leftmost (i.e.
 non-head words) the position 
\begin_inset Formula $k$
\end_inset

 is determined with respect to the position of the head word 
\begin_inset Formula $j$
\end_inset

 according to a distribution 
\begin_inset Formula $d_{>1}\left(k-j|class\left(f_{k}\right)\right)$
\end_inset

 .
\end_layout

\begin_layout Standard
Finally, the positions of all Null-generated words are distributed to the
 remaining slots uniformly [explain formula bit]
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
 & p\left(a,f|e\right)=\\
 & \prod_{i=1}^{l}n\left(\phi_{i}|e_{i}\right)\times\prod_{i=1}^{l}\prod_{k=1}^{\phi_{i}}t\left(\tau_{ik},e_{i}\right)\times\\
 & \prod_{i=1,\phi_{i}>0}^{l}d_{1}\left(\pi_{i1}-c_{\rho_{i}}|class\left(e_{\rho_{i}}\right),class\left(\tau_{i1}\right)\right)\times\\
 & \prod_{i=1}^{l}\prod_{k=2}^{\phi_{i}}d_{>1}\left(\pi_{ik}-\pi_{i\left(k-1\right)}|class\left(\tau_{ik}\right)\right)\times\\
 & \left(\begin{array}{c}
m-\phi_{0}\\
\phi_{0}\end{array}\right)p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-2\phi_{0}}\times\\
 & \prod_{k=1}^{\phi_{0}}t\left(\tau_{0k}|\text{NULL}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Integer Linear Programming Decoding
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\source}{\mathbf{e}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\target}{\mathbf{f}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\align}{\mathbf{a}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\start}{\text{START}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\stop}{\text{END}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\null}{\text{NULL}}
\end_inset


\end_layout

\begin_layout Standard
Given a trained IBM model 4, and an input target sentence 
\begin_inset Formula $\target$
\end_inset

 we need to find the source sentence 
\begin_inset Formula $\hat{\source}$
\end_inset

 and alignment 
\begin_inset Formula $\hat{\align}$
\end_inset

 with maximal 
\begin_inset Formula $p\left(\align,\source|f\right)\backsimeq p\left(\source\right)\cdot p\left(\align,\target|\source\right)$
\end_inset

.
 [german at all etc] showed that we can formulate this problem using Integer
 Linear Programming
\begin_inset space ~
\end_inset

[cite].
 In this section we will present our variant
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Our formulation differs slightly because we use a first order modelling
 language that imposed certain restrictions on the type of constraints allowed.
\end_layout

\end_inset

 of this ILP formulation.
 
\end_layout

\begin_layout Standard
[German et al] framed the search for the highest scoring translation (plus
 allignment) as the search for a single path over a set of source candidate
 tokens 
\begin_inset Formula $S$
\end_inset

.
 Let 
\begin_inset Formula $S_{i,10}$
\end_inset

 be the 10 most likely translations of 
\begin_inset Formula $\target_{i}$
\end_inset

 according to the probability 
\begin_inset Formula $p\left(e|f_{i}\right)$
\end_inset

.
 Then 
\begin_inset Formula $S$
\end_inset

 is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula \[
S=\left\{ s_{I}^{e}|I\neq\emptyset\wedge\forall i\in I:e\in S_{i,10}\right\} \cup\left\{ s_{\left\{ \start\right\} },s_{\left\{ \stop\right\} }\right\} \]

\end_inset

That is, 
\begin_inset Formula $S$
\end_inset

 contains one token 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 for each possible non-empty set 
\begin_inset Formula $I$
\end_inset

 of target tokens that can likely generate the source word 
\begin_inset Formula $e$
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Note that this search space restriction is based on a reversal of the noisy
 channel methaphor where source words generate target words.
 This approach is consistent with previous work [German], and signicantly
 reduces the noise that would appear in a heuristic based on 
\begin_inset Formula $p\left(f|e\right)$
\end_inset

 instead.
 However, it should be remembered that strictly speaking we (and [German])
 do not perform optimal decoding in IBM Model 4, but in a heuristically
 simplified version.
\end_layout

\end_inset

 In addition, 
\begin_inset Formula $S$
\end_inset

 contains a 
\begin_inset Formula $Start$
\end_inset

 and an 
\begin_inset Formula $End$
\end_inset

 token to indicate sentence beginning and end.
 For example, if a source word 
\begin_inset Formula $is$
\end_inset

 can generate 
\begin_inset Formula $f_{1}=CE$
\end_inset

 and 
\begin_inset Formula $f_{2}=NE$
\end_inset

 of the sentence 
\begin_inset Quotes eld
\end_inset

CE NE EST PAS CLAIR
\begin_inset Quotes erd
\end_inset

, the 
\begin_inset Formula $S$
\end_inset

 contains, among others, 
\begin_inset Formula $s_{\left\{ 1\right\} }^{is},s_{\left\{ 2\right\} }^{is}$
\end_inset

 and 
\begin_inset Formula $s_{\left\{ 1,2\right\} }^{is}$
\end_inset


\begin_inset Formula $ $
\end_inset

.
 
\end_layout

\begin_layout Standard
Each acyclic path 
\begin_inset Formula $\left(s_{\left\{ \start\right\} },s_{1},\ldots,s_{n},s_{\left\{ \stop\right\} }\right)$
\end_inset

 through 
\begin_inset Formula $S$
\end_inset

 that starts at 
\begin_inset Formula $Start$
\end_inset

 and ends at 
\begin_inset Formula $End$
\end_inset

 defines an English sentence 
\begin_inset Formula $\source$
\end_inset

 by sequentially appending all non-Null words 
\begin_inset Formula $e$
\end_inset

 of the 
\begin_inset Formula $s_{I}^{e}$
\end_inset

 tokens in the path.
 Such a path also defines an alignment 
\begin_inset Formula $\align$
\end_inset

 as follows: a token 
\begin_inset Formula $j=s_{I}^{e}$
\end_inset

 that appears in a path is aligned to all tokens 
\begin_inset Formula $i\in I$
\end_inset

 of the target sentence (in other words: source token 
\begin_inset Formula $j=s_{I}^{e}$
\end_inset

 generates all target tokens in 
\begin_inset Formula $I$
\end_inset

).
\end_layout

\begin_layout Standard
We describe a path through the source tokens 
\begin_inset Formula $S$
\end_inset

 using two types of variables.
 First, for each 
\begin_inset Formula $s\in S$
\end_inset

 we have a binary variable 
\begin_inset Formula $\alpha_{s}$
\end_inset

 that indicates whether token 
\begin_inset Formula $s$
\end_inset

 is part of the path.
 Second, we define a binary variable 
\begin_inset Formula $\sigma_{s,t}$
\end_inset

 for each 
\begin_inset Formula $s_{I},t_{J}\in S$
\end_inset

 with 
\begin_inset Formula $I\cap J=\emptyset$
\end_inset

 that indicates whether token 
\begin_inset Formula $s$
\end_inset

 is followed by token 
\begin_inset Formula $t$
\end_inset

 in the source sentence.
 
\end_layout

\begin_layout Description
Exactly
\begin_inset space ~
\end_inset

One
\begin_inset space ~
\end_inset

Source
\begin_inset space ~
\end_inset

Word For each target token 
\begin_inset Formula $i$
\end_inset

 there is exactly one source token 
\begin_inset Formula $s$
\end_inset

 that generates it 
\begin_inset Formula \[
\sum_{s_{I}^{e}|i\in I}\alpha_{s_{I}^{e}}=1\]

\end_inset


\end_layout

\begin_layout Description
No
\begin_inset space ~
\end_inset

Cycles For all cycles 
\begin_inset Formula $C\subseteq2^{S}$
\end_inset

 of source words 
\begin_inset Formula \[
\sum_{\left(s_{1},s_{2}\right)\in C}\sigma_{s_{1},s_{2}}\leq1\]

\end_inset


\end_layout

\begin_layout Description
Follows
\begin_inset space ~
\end_inset

Active
\begin_inset space ~
\end_inset

Consistency 
\begin_inset Formula \[
\alpha_{s_{I}^{e}}=\sum_{s}\sigma_{s_{I}^{e},s}=\sum_{s}\sigma_{s,s_{I}^{e}}\]

\end_inset


\end_layout

\begin_layout Description
Null
\begin_inset space ~
\end_inset

Target
\begin_inset space ~
\end_inset

Words We follow [German et.
 al] and require that there is at most a single active
\begin_inset Formula $\null$
\end_inset

 source token 
\begin_inset Formula $s_{I}^{\null}$
\end_inset

 , and that this token is not part of the actual path (or at its end).
 We omit the corresponding constraints for brevity.
\end_layout

\begin_layout Standard
Finally, the linear objective function incorporates the model 4 probabilities
 of a translation (as represented by an assignment for 
\begin_inset Formula $f$
\end_inset

 and 
\begin_inset Formula $a$
\end_inset

 variables) 
\begin_inset Formula \[
\sum_{t}\sum_{s\in S\left(t\right)}w_{s}^{t}a_{s}^{t}+\sum_{s_{1},s_{2}}w_{s_{1},s_{2}}f_{s_{1},s_{2}}\]

\end_inset

 where 
\begin_inset Formula $w_{s}^{t}$
\end_inset

 is 
\begin_inset Formula $\ldots$
\end_inset

 and 
\begin_inset Formula $w_{s_{1},s_{2}}$
\end_inset


\end_layout

\begin_layout Section
Cutting Plane Algorithm
\end_layout

\begin_layout Standard
The Integer Linear Program we have described above has an exponential number
 of (cycle) constraints.
 Hence, simply passing the ILP to an off-the-shelf ILP solver is not practical
 for all but the smallest sentences.
 For this reason the original 
\begin_inset Quotes eld
\end_inset

Optimal Decoding
\begin_inset Quotes erd
\end_inset

 work only considers sentences up to a length of 8? words.
 However, recent work [riedel&clarke] has shown that even exponentially
 large MAP problems can efficiently solved using ILP solvers if a so-called
 Cutting-Plane Algorithm is used.
 In the following we will present this algorithm in a nutshell.
\end_layout

\begin_layout Algorithm
Cutting Plane algorithm for MT
\end_layout

\begin_deeper
\begin_layout Enumerate
Construct ILP 
\begin_inset Formula $I$
\end_inset

 without cycle constraints
\end_layout

\begin_layout Enumerate

\series bold
do
\end_layout

\begin_deeper
\begin_layout Enumerate
solve 
\begin_inset Formula $I$
\end_inset

 and assign to 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_layout Enumerate
find cycles in 
\begin_inset Formula $I$
\end_inset

 solution
\end_layout

\begin_layout Enumerate
add corresponding cycle constraints to 
\begin_inset Formula $I$
\end_inset


\end_layout

\begin_layout Standard

\series bold
until
\series default
 no more cycles can be found
\end_layout

\end_deeper
\begin_layout Enumerate
return 
\begin_inset Formula $y$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
The Cutting Plane algorithm starts with a subset of the complete set of
 constraints, namely all constraints but the (exponentially many) cycle
 constraints.
 The corresponding ILP is solved by a standard ILP solver, and the solution
 
\begin_inset Formula $y$
\end_inset

 is inspected for cycles.
 If it contains no cycles we are done (we have found the true optimum: the
 solution with highest score that does not violate any constraints).
 If the solution does contain cycles, the corresponding constraints are
 added to the ILP which is in turn solved again.
 This process is continued until no more cycles can be found.
 
\end_layout

\begin_layout Standard
It is difficult to make claims about a guaranteed worst-case runtime (or
 number of iterations) of this algorithm.
 However, if the linear scoring function (in other words, the translation
 model and language model parameters) already provides a preference for
 cycle-free solutions, we can expect this algorithm to be efficient.
 For example, if we assume that the translation/distortion model has a very
 strong preference for monotonic solutions then clearly the highest scoring
 solution is likely to be cycle-free.
\end_layout

\begin_layout Section
Rewrite Decoder
\end_layout

\begin_layout Standard
Briefly describe...
\end_layout

\end_body
\end_document
