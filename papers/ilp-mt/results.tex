In this section we describe our experimental setup and results.

\subsection{Experimental setup}
\label{sec:experimental-setup}

Our experimental setup is designed to answer several questions: (1) Is
exact inference in IBM Model 4 possible for sentences of moderate
length? (2) How fast is exact inference using the cutting plane
method? (3) How well does the ReWrite decoder perform in terms of
finding the optimal solution? (4) Does exact decoding increase provide
better translations?

In order to answer these questions we obtain a trained IBM Model 4 for
French-English and German-English on Europarl version 3 using GIZA++.
A bigram language model with Witten-Bell smoothing was estimated from
the Europarl corpus using the CMU-Cambridge Language Modeling Toolkit.

For exact decoding we use the two models to generate ILP programs for
sentences of up to (and including) length 30 tokens for French and 25
tokens for German\footnote{These limits were imposed to ensure the
  Python script generating the ILP programs did not run out of
  memory.}.  We filter translation candidates following (TODO:CITE) by
using only the top ten translations for each word\footnote{Based on
  $t(e|f)$.} and a list of zero fertility words\footnote{Extracted
  using the rules in the filter script
  \texttt{rewrite.mkZeroFert.perl}}.  This resulted in 1101 French
sentences and 1062 German sentences for testing purposes.  The ILP
programs were then solved using the method described in
Section~\ref{sec:ilp}.  The same models were used with the ISI ReWrite
Decoder to solve the same set of sentences.


\subsection{Results}
\label{sec:results-results}

\begin{table*}[t!]
  \centering
  \subfloat[French-English\label{tab:results:french}] {
    \footnotesize
  \centering
  \begin{tabular}{|l|r|r|r|r|r|r|r|}
    \hline
    \multirow{2}{*}{Len} & \multirow{2}{*}{\#} & \multicolumn{3}{|c|}{Solve Stats} & \multicolumn{3}{|c|}{BLEU} \\
    & & \%gt & err & ST & ReW & ILP & Diff \\
    \hline
    1--5   & 21  & 47.6 & 0.15 & 0.7   & 56.47 & 56.15 &-0.32  \\
    6--10  & 121 & 52.1 & 0.08 & 1.4   & 26.11 & 28.01 & 1.90  \\
    11--15 & 118 & 62.8 & 0.06 & 2.7   & 22.85 & 23.70 & 0.85  \\
    16--20 & 238 & 67.6 & 0.06 & 13.9  & 20.40 & 20.81 & 0.41  \\
    21--25 & 266 & 74.8 & 0.07 & 70.1  & 20.89 & 22.51 & 1.62  \\
    26--30 & 152 & 77.6 & 0.05 & 162.6 & 20.92 & 22.30 & 1.38  \\
    \hline                                    
    1--30  & 986 & 67.9 & 0.06 & 48.1  & 21.66 & 22.63 & 0.97  \\
    \hline
  \end{tabular}
}
\subfloat[German-English\label{tab:results:german}] {
  \centering
  \footnotesize
  \begin{tabular}{|l|r|r|r|r|r|r|r|}
    \hline
    \multirow{2}{*}{Len} & \multirow{2}{*}{\#} & \multicolumn{3}{|c|}{Solve Stats} & \multicolumn{3}{|c|}{BLEU} \\
    & & \%gt & err & ST & ReW & ILP & Diff \\
    \hline
    1--5   & 31   & 100 & 0.27 & 0.8   & 40.68 & 41.12 & 0.44  \\
    6--10  & 175  & 100 & 0.20 & 1.7   & 19.19 & 20.91 & 1.72  \\
    11--15 & 242  & 100 & 0.17 & 5.5   & 15.97 & 16.69 & 0.72  \\
    16--20 & 257  & 100 & 0.14 & 23.9  & 15.78 & 15.94 & 0.16  \\
    21--25 & 249  & 100 & 0.14 & 173.4 & 15.31 & 15.92 & 0.61  \\
    \hline                              
    1--25  & 954  & 100 & 0.16 & 53.5  & 16.10 & 16.71 & 0.61   \\
    \hline
  \end{tabular}
}
\caption{\footnotesize Results on the two corpora.  Len: range of sentence lengths; \#: number of sentences in this range; \%gt: percentage of times ILP decoder returned higher result than ReWrite (all other times equal); err: average difference between decoder scores per token; ST: the average solve time per sentence of ILP decoder in seconds; BLEU ReW, BLEU ILP, BLEU Diff: the BLEU scores of the output and difference between BLEU scores.}  \label{tab:comparison}
\end{table*}

% TODO: difficult to decide how strong our claims can be regarding is
% ILP-M4 possible?
The ILP decoder produced output for 986 French sentences and 954
German sentences.  From this we can conclude that it is possible to
solve 90\% of our sentences exactly using ILP.  For the remaining 115
and 108 sentences we did not produce a solution due to: (1) the solver
not completing within 30 minutes, or (2) the solver running out of
memory.
%TODO: Maybe we just leave this out? as it is a quirk/technical detail
%or (3) the solver producing a one word output due to


Table~1 shows a comparison of the results obtained on the 986 French
and 954 German sentences using the ILP and ReWrite decoders.  The
results are broken down by sentence length range of the input
sentence.  We now turn our attention to the solve times obtained using
ILP (for the sentences for which the solution was found 30 minutes).
The table shows that the average solve time is under a minute per
sentence.  As we increase the sentence length we see the solve time
increases, however, we never see an order of magnitude increase
between brackets as witnessed in (TODO: Cite) thus exact decoding is
more practical than previously suggested.

We next examine the model scores.  The Table shows in the French case
the ReWrite decoder finds the optimal solution 32.1\% of the time for
French and 0\% for German.  Although the same English string is
produced for 40.1\% of the French sentences and 29.1\% of the German
sentences\footnote{This could be due to precision and rounding errors
  in comparing the model scores, we call two log model scores equal if
  they are within 0.1 of one another.}.  Examining the average log
model error per token shows that the ReWrite decoder error is
proportional to sentence length (TODO: Can we really say this?).  On
average the ReWrite decoder is 2.2\% away from the optimal solution in
log space and 60.6\% in probability space\footnote{These high error
  rates are an artefact of the extremely small probabilities
  involved.} for French, and 4.7\% and 60.9\% for German.

Performing exact decoding increases the BLEU score by 0.97 points on
the French-English data set and 0.61 points on the German-English data
set with similar performance increases observed for all sentence
lengths.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "ilp-mt"
%%% End: 
