forall x : pred(x) <=> ! exists x : !pred(x) <=> |x:!pred(x)| <= 0

s = signature {

}

m = model(s) {
  factor : Int t, Slot s add [slot(t,s)] * 123.123;
}

m         = normalize(m);
w         = weights(m,s);
corpus    = corpus("$filename", s);
collector.collect(corpus,w);
instances = instances(corpus,"$2");
learner   = onlineLearner(w);
learner.solver.ground.asLeastOne = true;
learner.learn(instances);
w.save("$3");

gold  = corpus.get(0);
guess = solver.solve();

result = conll("w","result.conll");
result.append(guess);

//or
solve.solve(corpus,result);



corpus = conll("$1");
gold = ram(corpus,1,100);
guess = solver.solve(gold);
instances = instances(ram,int($2));
learner.solver.model = sat();
learner.solver.model.solver = maxWalkSat();
collector.collect(corpus,weights);

collector = default();

print learner.solver;

query guess {Int t, Word w: word(t,w)};
query guess {Word w, Double s: s=w_word(w) & word(_,w)};
query {Token t, Pos p, Double s: s=<pos(t,p>};

collector.collect(corpus);
learner.learn(instances);

next(5);
solver.solve();



|Slot s: slot(t,s)| >= |Slot s|


Nips:

Core:
Exact vs Approximate
- Score + F1: ICP-ILP vs MWS (with good parameters, for datasets 50 - 150?)

ILP Speed vs MWS
- Speed: Full-MWS vs Full-ILP, MWS vs (I)CP-ILP
- overhead + iterations separately

ILP Speed: Full vs CP vs ICP
- Speed (iterations/overhead): CP-ILP vs ILP vs ICP-ILP


Add-On:
- Score + F1: CP-MWS vs MWS (what maxFlip value?)
- Score + F1: CP-ILP vs CP-MWS
-


class IntData {
}

class DoubleData {
}

class StringData {

}

class Table {
  int[] intData;
  String[] stringData;
  double[] doubleData;
  
}



class Runtime {
  - Corpus (for training and testing)
  - RamCorpus (for inspection)
  - Learner
  - Solver
  - Feature Collector
  - Current
    - index in ram corpus
    - gold atoms
    - guess atoms

}


chunk features (pos):

 |WORD| (word is not rare)
 |contained in gazetteer| 

chunk features (pos):

+ POS|POS ... POS|POS  until length 6

+ |POS POS|
+ |POS POS POS|
+ |POS POS POS POS|

- |... POS ...|
- |... POS POS ...|


w=(w_s,w_h)

w * df > L
w_s * df > L


f = feature vector of violation
f = f1 + f2 (f1 and f2 don't violate)

(fg-f) * w > L

(fg-(f1+f2)) * w > L

(fg - f1 - f2) * w > L
fg * w - f1 * w - f2 * w > L
(fg-f1)*w - f2 * w > L



44,44,12,6,2

85:312844
29:159420
781:609532
317:272356
327:88000
139:42952
1767:450500
792:216624
463:312844
212:159420
86:609532
58:272356
119:88000
50:42952
593:450500
285:216624
.1229:312844
590:159420
2400:609532
989:272356
.
index: dep(*,*,_);

to conll06 "test.conll";

load corpus.gold from "...";
load corpus.guess from "...";

save corpus to 

test to conll06 "test.conll"; //goes over the gold corpus and create a guess corpus;
test to ram;

TheRuntime: will contain runtime object


1. Model
2. Corpus
3. Gold atoms (if corpus has hidden atoms)
4. Guess atoms
5. Training instances (if produced before)
6. Scores
7. RAM Corpus (for playing around)




//projectivity
factor: for Int h1, Int m1, Int h2, Int m2: if h1 < h2 & m1 > h2 & m2 > m1 :!(link(h1,m1) & link(h2,m2));
factor: for Int h1, Int m1, Int h2, Int m2: if h1 > h2 & m1 < h2 & m2 > h1 :!(link(h1,m1) & link(h2,m2));


load corpus from conll06 "asdasd";
save corpus (1-100) to ram;
save corpus (1-100) to dump "dump.txt" (100);
save corpus (1-100) to text "text.txt";

corpus from dump "dump.txt";

set guess = load("asdasd");
set guess = greedy();
set scores = score();
set scores = load("scores");

Dump dump = server.createDump("/tmp/dmp");

dump.save(var1);
dump.save(var2);
dump.close();
dump.load(var);

groundatoms:
dump.save(count);
dump.save(relation);
